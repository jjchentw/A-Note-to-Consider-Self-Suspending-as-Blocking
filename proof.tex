\ifpaper
\newtheorem{property}{Property}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\fi
\section{Proof of Liu's Analysis}  

This section provides the proof to support the correctness of the test in Eq. \eqref{eq:TDA-suspension}. First, it should be easy to see that we can convert the suspension time of task $\tau_k$ into computation. This has been proven in many previous works, e.g., Lemma~3 in \cite{Liu_2014} and Theorem~2 in \cite{ecrts15nelissen}. Yet, it remains to formally prove that the additional interference due to the self-suspension of a higher-priority task $\tau_i$ is upper-bounded by $b_i=min(C_i, S_i)$. The interference to be at most $C_i$ has been provided in the literature as well, e.g., \cite{Rajkumar_1990,Liu_2014}. However, the argument about blocking task $\tau_k$ due to a higher-priority task $\tau_i$ by at most $S_i$ amount of time is not straightforward. 

From the above discussions, we can greedily convert the suspension time of task $\tau_k$ to its computation time. For the sake of notational brevity, let $C_k'$ be $C_k + S_k$. We call this converted version of task $\tau_k$ as task $\tau_k'$. Our analysis is also based on very simple properties and lemmas enunciated as follows:

\begin{property}
\label{prop:lower-priority}
In a preemptive fixed-priority schedule, the lower-priority jobs do not impact the schedule of the higher-priority jobs.
\end{property}

%\begin{lemma}
%\label{lemma:remove-lower-priority}
%In a preemptive fixed-priority schedule, removing a lower-priority job arrived at time $t$ does not affect the schedule of the higher-priority jobs after time $t$.
%\end{lemma}
%\begin{proof}
%This is a direct consequence of Property~\ref{prop:lower-priority}.
%\end{proof}

\begin{lemma}
\label{lemma:remove-same-task}
In a preemptive fixed-priority schedule, if the worst-case response time of task $\tau_i$ is no more than its period $T_i$, removing a job of task $\tau_i$ does not affect the schedule of any other job of task $\tau_i$.
\end{lemma}
\begin{proof}
Since the worst-case response time of task $\tau_i$ is no more than its period, any job $\tau_{i,j}$ of task $\tau_i$ completes its execution before the release of the next job $\tau_{i,j+1}$. Hence, the execution of $\tau_{i,j}$ does not interfere with the execution of any other job of $\tau_i$, which then depends only on the schedule of the higher priority jobs. Furthermore, as stated in Property~\ref{prop:lower-priority}, the removal of $\tau_{i,j}$ has no impact on the higher-priority jobs, thereby implying that the other jobs of task $\tau_i$ are not affected by the removal of $\tau_{i,j}$.
\end{proof}

We can prove the correctness of Eq. \eqref{eq:TDA-suspension} by using a similar proof of the critical instant theorem of the ordinary sporadic task system.
Let $R_k'$ be the minimum $t > 0$ such that  $C'_k + B_k + \sum_{i=1}^{k-1}\ceiling{\frac{t}{T_i}} C_i = t$, i.e., Eq. \eqref{eq:TDA-suspension} holds. The following lemma shows that $R_k'$ is a safe upper bound if the worst-case response time of task $\tau_k'$ is no more than $T_k$.

\begin{theorem}
\label{theorem:critical}
 $R_k'$ is a safe upper bound on the worst-case response time of task $\tau_k'$ if the worst-case response time of $\tau_k'$ is not larger than $T_k$.
\end{theorem}
\begin{proof}
Let us consider the task set $\tau'$ composed of $\left\{\tau_1, \tau_2, \ldots, \tau_{k-1}, \tau_k', \tau_{k+1}, \ldots \right\}$ and let $S$ be a schedule of $\tau'$ that generates the worst-case response time of $\tau_k'$. Suppose that the job $J_{k}$ of task $\tau_k'$ with the largest response time in $S$ arrives at time $r_k$ and finishes at time $\rho$. We know by Property~\ref{prop:lower-priority} that the lower priority tasks $\tau_{k+1}, \tau_{k+2}, \ldots$ do not impact the response time of $J_{k}$. Moreover, since we assume that the worst-case response time of task $\tau_k'$ is no more than $T_k$, Lemma~\ref{lemma:remove-same-task} proves that removing all the jobs of task $\tau_k'$ but $J_{k}$ has no impact on the schedule of $J_{k}$ arrived at time $r_k$. Therefore, let $S^{red}$ be a schedule identical to $S$ but removing all the jobs released by the lower priority tasks $\tau_{k+1}, \tau_{k+2}, \ldots$ as well as all the jobs released by $\tau_k'$ at the exception of $J_{k}$. The response time of $J_{k}$ in $S^{red}$ is identical to the response time of $J_{k}$ in $S$.


Therefore, for the rest of the proof, we only have to consider this \emph{reduced} schedule $S^{red}$. Note that by construction of $S^{red}$, the processor is busy only when executing higher-priority tasks than $\tau'_k$ or the job of task $\tau_k'$ released at time $r_k$ and completing at time $\rho$. In $S^{red}$, let $t_{k}$ be the latest moment before $r_k$ such that the processor does not execute any job. That is, from $t_k$ to $r_k$, the processor executes tasks with higher priorities than $\tau_k'$. Apparently, one can change the release time of the unique job of task $\tau_k'$ in $S^{red}$ to time $t_k$, and hence increase the response time of the job to $\rho-t_k \geq \rho-r_k$. It however contradicts our assumption that the response time of $J_{k}$ is the worst-case response time of $\tau_k'$. Consequently, $t_k$ is equal to the release $r_k$ of $J_{k}$ in $S^{red}$ and the processor is idle before $t_k$.

Up to here, the proof is basically similar to the proof of the critical instant theorem of the usual sporadic sequential real-time task model. However, for self-suspending tasks, one needs to consider that a job of a higher priority task $\tau_i$ can suspend itself before $t_k$ and resume its execution after $t_k$. Such jobs are usually referred to as \emph{carry-in} jobs. %Fortunately, each higher-priority task has only one carry-in job due to the assumption that the higher-priority tasks are assumed to finish before their periods. However, analyzing the accurate workload of such jobs due to self-suspension is non-trivial. 
%One can conclude that each job of task $\tau_i$ has execution time up to $C_i$. This is fine with $S_i \geq C_i$. If $S_i < C_i$, we explain how to further extend the analysis window further iteratively. For the simplicity of presentation, let $J_i$ be the carry-in job of task $\tau_i$ at time $t_k$.

The proof is built upon the two following steps:
\begin{enumerate}
\item First, we extend the window of interest from $[t_k, \rho)$ to a larger time window $[t_1, \rho)$ by iteratively inspecting the schedule of the higher priority tasks in $S^{red}$, starting with $\tau_{k-1}$ until the highest priority task $\tau_1$. At each iteration, the window of interest is extended from $[t_{j+1}, \rho)$ to $[t_{j}, \rho)$ with $t_j \leq t_{j+1}$ ($1 \leq j < k$). Once $t_j$ has been identified, all the jobs of task $\tau_j$ released before $t_j$ are removed and, if needed, replaced by an artificial job to consider the residual workload of task $\tau_j$ at time $t_j$ impacting the response time of $J_{k}$. 
\item Second, the final reduced schedule is analyzed in the time window $[t_1, \rho)$ to characterize the response time of $J_{k}$ and hence the worst-case response time of $\tau_k$.
\end{enumerate}

{\bf Step 1: Extending the Window of Interest} 

Let $S^k$ be a schedule exactly identical to $S^{red}$. During this step, we iteratively build an artificial schedule $S^j$ from $S^{j+1}$ (with $1 \leq j < k$) so that the response time of $J_{k}$ can only increase. As already discussed above, let $t_k$ be equal to $r_k$ in $S^k$. At each iteration, we define $t_j$ for task $\tau_j$ in the schedule $S^{j+1}$ (with $j=k-1, k-2, \ldots, 1$) and build $S^j$ by removing all the jobs released by $\tau_j$ before $t_j$.


Let $r_j$ be the arrival time of the last job released by $\tau_j$ before $t_{j+1}$ in $S^{j+1}$ and let $J_{j}$ denote that job. %There are a two possible cases:
%\begin{itemize}
%\item $J_{j}$ completed its execution no later than $t_{j+1}$. Then, we simply set $t_j$ to $t_{j+1}$ and generate $S^j$ by removing all the jobs of task $\tau_j$ arrived before $t_{j+1}$ in the schedule $S^{j+1}$. By Lemma~\ref{lemma:remove-same-task} and Property \ref{prop:lower-priority}, removing all the jobs of task $\tau_j$ arrived before $t_{j+1}$ has no impact on the schedule of the higher-priority jobs (jobs released by $\tau_1, \ldots, \tau_{j-1}$) and the jobs of $\tau_j$ released after $t_{j+1}$. Moreover, because no task with a priority lower than $\tau_j$ executes jobs before $t_{j+1}$ in $S^{j+1}$, removing the jobs released by $\tau_j$ before $t_{j+1}$ does not impact the schedule of the jobs of $\tau_{j+1}, \ldots, \tau_{k}$. The response time of $J_{k}$ in $S^j$ thus remains unchanged in comparison to its response time in $S^{j+1}$. 
%\item $J_{j}$ did not complete its execution before $t_{j+1}$.
Removing all the jobs of task $\tau_j$ arrived before $r_j$ has no impact on the schedule of any other job released by $\tau_j$ (Lemma~\ref{lemma:remove-same-task}) or any higher priority job released by $\tau_1, \ldots, \tau_{j-1}$ (Property \ref{prop:lower-priority}). Moreover, because by construction of $S^{j+1}$, no task with a priority lower than $\tau_j$ executes jobs before $t_{j+1}$ in $S^{j+1}$, removing the jobs released by $\tau_j$ before $t_{j+1}$ does not impact the schedule of the jobs of $\tau_{j+1}, \ldots, \tau_{k}$. Therefore, we can safely remove all the jobs of task $\tau_j$ arrived before $r_j$ without impacting the response time of $J_{k}$. Two cases must then be considered:
\begin{enumerate}[(a)]
\item $\tau_j \in {\bf T}_1$, i.e., $S_j < C_j$. For such a case, $t_{j}$ is set to $r_j$ and hence $S^j$ is built from $S^{j+1}$ by removing all the jobs released by $\tau_j$ before $r_j$. Note that because by construction of $S^{j+1}$ and hence $S^j$ there is no job with priority lower than $\tau_j$ available to be executed before $t_{j+1}$, the maximum amount of time during which the processor remains idle within $[t_j, t_{j+1})$ is at most $S_j$ time units.
\item $\tau_j \in {\bf T}_2$, i.e., $S_j \geq C_j$. For such a case, we set $t_{j}$ to $t_{j+1}$. Let $c_j(t_j)$ be the remaining execution time for the job of task $\tau_j$ at time $t_j$. We know that $c_j(t_j)$ is at most $C_j$. Since by construction of $S^j$, all the jobs of $\tau_j$ released before $t_j$ are removed, the job of task $\tau_j$ arrived at time $r_j$ ($< t_j$) is replaced by a new job released at time $t_j$ with execution time $c_j(t_j)$ and the same priority than $\tau_j$. Clearly, this has no impact on the execution of any job executed after $t_j$ and thus on the response time of $J_k$. The remaining execution time $c_j(t_j)$ of $\tau_j$ at time $t_j$ is called the \emph{residual workload} of task $\tau_j$ in the rest of the proof.
\end{enumerate}
%\end{itemize}
 
%The above construction of $t_{k-1}, t_{k-2}, \ldots, t_1$ is well-defined. After each iteration to set $t_j$, we can reduce the schedule by removing some jobs without affecting the schedule of the carry-in $J_j$. (Note that $J_j$ is defined as the carry-in job of task $\tau_j$ at time $t_k$.) Therefore, the reduced schedule after the above procedure does not change the execution of $J_j$ after time $t_j$ if $\tau_j$ is in ${\bf T}_1$. For a task $\tau_j$ in ${\bf T}_2$, its corresponding carry-in job $J_j$ may be changed, but its execution after $t_j$ remains identical as in the original schedule. 
%Therefore, the resulting schedule above does not change any execution behavior of the (at most) $k-1$ carry-in jobs at time $t_k$.

{\bf Step 2: Analyze the Final Reduced Schedule in $[t_1, \rho)$:}


We now analyze the properties of the final schedule $S^1$ in which all the unnecessary jobs have been removed.   

From case (b) of Step 1, the total residual workload that must be considered in $S^1$ is upper bounded by $\sum_{\tau_i \in {\bf T}_2} C_i$.
%Consequently, for any time $t$ such that $t_1 < t \leq \rho$, the total amount of idle time and residual workload within $[t_1, t)$ is upper bounded by $\sum_{\tau_i \in {\bf T}_1} S_i + \sum_{\tau_i \in {\bf T}_2} C_i = \sum_{i=1}^{j} b_i$. 
Therefore, considering the fact that no job of $\tau_j$ is released before $t_j$ in $S^1$ ($j=1,2,\ldots,k-1$), the workload released by the tasks within any time interval $[t_1, t)$ such that $t_1 < t \leq \rho$ is upper bounded by 
\begin{align*}
\sum_{i=1}^k \left( c_j(t_j) + \max\{0, \ceiling{\frac{t- t_i}{T_i} } C_i \} \right) & \leq \sum_{\tau_i \in {\bf T}_2} C_i + \sum_{i=1}^k \max\{0, \ceiling{\frac{t- t_i}{T_i} } C_i \}
\end{align*}
%\[
%\forall t_j \leq t < t_{j+1},\qquad  \sum_{i=1}^{j} b_i + \sum_{i=1}^j %\ceiling{\frac{t- t_i}{T_i} } C_i >  t-t_1.
%\]
%By further considering the time interval from $t_k$ to $\rho$, we have
%\[
%\forall t_k \leq t < \rho,\qquad  C_k'+\sum_{i=1}^{k-1} b_i + \sum_{i=1}^{k-1} \ceiling{\frac{t- t_i}{T_i} } C_i > t-t_1.
%\]

Furthermore, from case (a) of Step 1, we know that the maximum amount of time during which the processor is idle in $S^1$ within any time interval $[t_1, t)$ such that $t_1 < t \leq t_k$, is upper bounded by $\sum_{\tau_i \in {\bf T}_1} S_i$. %We can simply consider such self-suspension time as \emph{virtual computation}.
Hence, adding that time to the maximum workload released by the tasks within any time interval $[t_1, t)$ such that $t_1 < t \leq t_k$, it holds that
\[
\forall t \mid t_1 \leq t < t_k,\qquad  \sum_{\tau_i \in {\bf T}_1} S_i + \sum_{\tau_i \in {\bf T}_2} C_i + \sum_{i=1}^{k} \max\{ 0, \ceiling{\frac{t- t_i}{T_i} } C_i\}  \geq t-t_1
\]
Considering that $C_k' > 0$ and that $\max\{ 0, \ceiling{\frac{t- t_k}{T_k} } C_k\} = 0$ for any $t$ smaller than $t_k$, we get that
\[
\forall t \mid t_1 \leq t < t_k,\qquad  \sum_{\tau_i \in {\bf T}_1} S_i + \sum_{\tau_i \in {\bf T}_2} C_i + C_k' + \sum_{i=1}^{k-1} \max\{ 0, \ceiling{\frac{t- t_i}{T_i} } C_i\}  > t-t_1
\]
and simplifying using the definition of $b_i$
\begin{equation}
\label{eq:eq1_in_proof}
\forall t \mid t_1 \leq t < t_k,\qquad  C_k'+\sum_{i=1}^{k-1} b_i + \sum_{i=1}^{k-1} \max\{ 0, \ceiling{\frac{t- t_i}{T_i} } C_i\} \geq t-t_1
\end{equation}

Additionally, because $J_k$, which is released at time $t_k$, does not complete its execution before $\rho$, it must hold that
\begin{equation}
\label{eq:eq1_in_proof}
\forall t \mid t_k \leq t < \rho,\qquad  C_k'+\sum_{i=1}^{k-1} b_i + \sum_{i=1}^{k-1} \max\{ 0, \ceiling{\frac{t- t_i}{T_i} } C_i\} > t-t_1.
\end{equation}

Since $t_i \geq t_1$ for $i=1,2,\ldots,k$, there is 
$$\ceiling{\frac{t- t_i}{T_i} } \leq \ceiling{\frac{t- t_1}{T_i} }$$ and without any loss of generality, by arbitrarily assuming $t_1 =0$, Inequation~\ref{eq:eq1_in_proof} becomes
\[
\forall t \mid 0 < t < \rho, \qquad C_k'+\sum_{i=1}^{k-1} b_i + \sum_{i=1}^{k-1} \ceiling{\frac{t}{T_i} } C_i > t.
\]
The above inequation implies that the minimum $t$ such that $C_k'+\sum_{i=1}^{k-1} b_i + \sum_{i=1}^{k-1} \ceiling{\frac{t}{T_i} } C_i \leq t$ is larger than or equal to $\rho$. And because by assumption the worst-case response time of $\tau_k'$ is equal to $\rho-t_k$ which is obviously smaller than or equal to $\rho$, it holds that $R_k'$ is a safe upper bound on the worst-case response time of $\tau_k'$.
\end{proof}

\begin{corollary}
\label{cor:critical}
 $R_k'$ is a safe upper bound on the worst-case response time of task $\tau_k'$ if $R_k'$ is not larger than $T_k$.
\end{corollary}
\begin{proof}
Directly follows from Theorem~\ref{theorem:critical}.
\end{proof}



\begin{corollary}
$R'_k$ is a safe upper bound on the worst-case response time of task $\tau_k$ if $R_k'$ is not larger than $T_k$. 
\end{corollary}
\begin{proof}
Since, as proven in \cite{Rajkumar_1990,Liu_2014}, the worst-case response time of $\tau_k'$ is always larger than or equal to the worst-case response time of $\tau_k$, this corollary directly follows from Corollary~\ref{cor:critical}. 
\end{proof}


  
To illustrate Step 1 in the above proof, we also provide one concrete example. Consider a task system with the following 4 tasks:
\begin{itemize}
\item $T_1 = 6, C_1 = 1, S_1 = 1$,
\item $T_2 = 10, C_2 = 1, S_2 = 6$,
\item $T_3 = 18, C_3 = 4, S_3 = 1$,
\item $T_4 = 20, C_4 = 5, S_4 = 0$.
\end{itemize}

Figure~\ref{fig:example} demonstrates a schedule for the jobs of the
above 4 tasks. We assume that the first job of task $\tau_1$ arrives
at time $4+\epsilon$ with a very small $\epsilon > 0$. The first job
of task $\tau_2$ suspends itself from time $0$ to time $5+\epsilon$,
and is blocked by task $\tau_1$ from time $5+\epsilon$ to time
$6+\epsilon$. After some very short computation with $\epsilon$ amount
of time, the first job of task $\tau_2$ suspends itself again from
time $6+2\epsilon$ to $7$.   In this schedule, $\rho$ is set to $20-\epsilon$.

We define $t_4$ as $7$. Then, we set $t_3$ to $6$. When considering
task $\tau_2$, since it belongs to ${\bf T}_2$, we greedily set $t_2$
to $t_3=6$ and the residual workload $C_2'$ is $1$. Then, $t_1$ is set
to $4+\epsilon$. In the above schedule, the idle time from
$4+\epsilon$ to $20-\epsilon$ is at most $2 = S_1+S_3$. We have to
further consider one job of task $\tau_2$ arrived before time $t_1$
with execution time $C_2$.
  
  
  

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "authorea_build/authorea_paper.tex"
%%% End:
