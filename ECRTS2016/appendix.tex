\section*{Appendix}
{\bf How Rajkumar, Sha, and Lehoczky
  in~\cite[p. 267]{DBLP:conf/rtss/RajkumarSL88} analyzed dynamic
  self-suspending behaviour due to multiprocessor synchronization?}
The statement in \cite{DBLP:conf/rtss/RajkumarSL88} reads as follows:
\begin{quote}
\emph{``For each higher priority job $\tau_{i,j}$ that suspends on global semaphores or for other reasons, add the term $\min(C_i, S_i)$ to $B_k$, where $S_i$ is the maximum duration that $\tau_{i,j}$ can suspend itself. [...] The sum [...] yields $B_k$, which in turn can be used in 
$\frac{C_k+B_k}{T_k} + \sum_{i=1}^{k-1} U_i \leq k (2^{\frac{1}{k}}-1)$ to determine whether the current task allocation to the processor is schedulable."}
\end{quote}
  We rephrased the wording and notation in order to be consistent with this paper. Moreover, the multiprocessor scheduling in such a case is based on partitioned scheduling. Therefore, the schedulability analysis of a task set on a processor is the same as the uniprocessor problem by additionally considering the self-suspending behaviour due to the synchronization with other tasks on other processors.

\begin{appProof}{of Lemma~\ref{lemma:remove-same-task}}
Since, by assumption, the worst-case response time of task $\tau_i$ is no more than its period, any job $\tau_{i,j}$ of task $\tau_i$ completes its execution before the release of the next job $\tau_{i,j+1}$. Hence, the execution of $\tau_{i,j}$ does not directly interfere with the execution of any other job of $\tau_i$, which then depends only on the schedule of the higher priority jobs. Furthermore, as stated in Property~\ref{prop:lower-priority}, the removal of $\tau_{i,j}$ has no impact on the schedule of the higher-priority jobs, thereby implying that the other jobs of task $\tau_i$ are not affected by the removal of $\tau_{i,j}$.
\end{appProof}

{\bf Transformation from $\Psi$ to $\Psi^1$:}
Here, we present the pseudo-code to transform from the given schedule $\Psi$
to $\Psi^1$:
\begin{algorithm}[h]
  \caption{Transformation from $\Psi$ to $\Psi^1$}
     \begin{algorithmic}[1]\footnotesize
       \INPUT $\tau_k'$, ${\bf T}_0$, ${\bf T}_1$, and a
       fixed-priority preemptive schedule $\Psi$ of $\tau'$ under the
       assumption $R_k' \leq T_k$;

       \STATE pick one job $J_k$ of task $\tau_k'$ and set $r_k$ as
       the arrival time of $J_k$;

    \STATE remove all the jobs generated from $\tau_k', \tau_{k+1}, \tau_{k+2}, \ldots,
    \tau_n$ in the schedule $\Psi$, except $J_k$;

    \STATE $\Psi^k \leftarrow \Psi$ and $t_k \leftarrow r_k$; 

    \FOR {$j \leftarrow k-1$ to $1$}
    
    \STATE let $r_j$ be the arrival time of the last
    job released by $\tau_j$ before $t_{j+1}$ in $\Psi^{j+1}$ and let
    $J_{j}$ denote that job;

    \IF {$r_j$ does not exist} 
    \STATE $\Psi^j \leftarrow \Psi^{j+1}$ and $t_j \leftarrow t_{j+1}$; 
    \ELSE
    \STATE $\Psi^j \leftarrow \Psi^{j+1}$ and remove all the jobs of task $\tau_j$ released before $r_j$
    in schedule $\Psi^j$;
     \IF {$\tau_j \in {\bf T}_0$} 
     \STATE  $t_j \leftarrow t_{j+1}$, remove $J_j$, and  create an
     artificial job to represent the residual workload of $J_j$,
     executed at or after $t_{j+1}$; \COMMENT {\bf Rule 3}
     \ELSE
     \IF {$J_j$ completes its execution at or before $t_{j+1}$}
     \STATE  $t_j \leftarrow t_{j+1}$, remove $J_j$ in schedule $\Psi^j$; \COMMENT {\bf Rule 2}
     \ELSE
     \STATE  $t_j \leftarrow r_j$; \COMMENT {\bf Rule 1}
     \ENDIF   
    \ENDIF
    \ENDIF
    \ENDFOR
    \STATE return $\Psi^1$;
  \end{algorithmic}
\end{algorithm}

\begin{appProof}{of Lemma~\ref{lemma:Wj0-dominate}}
  We first prove that $\widehat{W}_j^0(\Delta, C_j) \geq \widehat{W}_j^0(\Delta, c_j^*)$ when $c_j^* =0$. That is, we prove that $\widehat{W}_j^0(\Delta, C_j) \geq W_j^1(\Delta)$ (see Eq.~\eqref{eq:execution-case2-precise}). 
  
  By definition, $\rho_j=T_j-R_j+C_j$ when $c_j^*$ is $C_j$. Because by
 assumption $C_j \leq R_j \leq T_j$, we have $0 \leq \rho_j \leq
 T_j$. Therefore, for $\Delta \geq T_j$, we have $W_j^1(\Delta) = C_j
 + W_j^1(\Delta-T_j) \leq C_j + W_j^1(\Delta - \rho_j) =
 \widehat{W}_j^0(\Delta, C_j)$ where the last equality is given by the fourth case of Eq.~\eqref{eq:execution-case2-precise} when $c_j^* = C_j$. For $0 \leq \Delta < T_j$, it is also
 obvious that $\widehat{W}_j^0(\Delta, C_j) \geq \min\{\Delta, C_j\}
 = W_j^1(\Delta)$.

 We then prove that $\widehat{W}_j^0(\Delta, C_j) \geq
 \widehat{W}_j^0(\Delta, c_j^*)$ for any $0 < c_j^* \leq C_j$ based on its definition in Eq.~\eqref{eq:execution-case2-precise}. 
Figure~\ref{fig:example-lemma-workload} provides an illustrative example for $\widehat{W}_j^0(\Delta, c_j^*)$. We consider three subcases:
 \begin{compactitem}
 \item For $0 \leq \Delta \leq C_j$, it is obvious that
   $\widehat{W}_j^0(\Delta, C_j) \geq \widehat{W}_j^0(\Delta,
   c_j^*)$.
 \item For $C_j < \Delta \leq T_j-R_j+C_j$, we have
   $\widehat{W}_j^0(\Delta, C_j) = C_j$, and from Eq~\eqref{eq:execution-case2-precise},
   $\widehat{W}_j^0(\Delta, c_j^*) = c_j^* + \max\{0,
   \Delta-(T_j-R_j+c_j^*)\} \leq c_j^* + C_j - c_j^* = C_j$.

 \item For $T_j-R_j+C_j < \Delta$, we have $\widehat{W}_j^0(\Delta,
   C_j) = C_j + W_j^1(\Delta - (T_j-R_j+C_j))$.  Moreover, by
   definition, we also know $W_j^1(\Delta, c_j^*) \leq
   \delta+W_j^1(\Delta-\delta, c_j^*)$ for any $\delta$ such that $0 < \delta \leq
   \Delta$. Therefore, we conclude that
   $\widehat{W}_j^0(\Delta, c_j^*) = c_j^* + W_j^1(\Delta -
   (T_j-R_j+c_j^*)) \leq C_j + W_j^1(\Delta - (T_j-R_j+C_j))$ by
   setting $\delta$ to $C_j - c_j^*$ in the previous inequality.
 \end{compactitem}
\end{appProof}


\begin{figure}[t]
  \centering

		\def\ux{0.32cm}\def\uy{0.3cm} 
		\begin{tikzpicture}[x=\ux,y=\uy, font=\sffamily,thick]  
		\draw[->] (0,0) -- coordinate (xaxis) (21,0) node[anchor=west] {$\Delta$};
		\foreach \x in {0,...,20}{
			\draw[-](\x,0.1) -- (\x,-0.1)
			node[below] {\footnotesize $\x$};
			
		}
		\draw[->] (0,0) node[anchor=east] {}-- coordinate (xaxis) (0,10.5);
		\foreach \y in {0,...,10}{
			\draw[-](0.1, \y) -- (-0.1, \y)
			node[left] {\small $\y$};
			
		}

                \draw[-,thick] (0, 0) -- (3, 3) -- (7, 3) -- (10, 6) -- (17, 6) -- (20, 9);
                \draw[-,dashed] (0 ,0) -- (2,2) -- (6, 2) -- (9, 5) -- (16, 5) -- (19, 8) -- (20, 8);
                \draw[-,dotted] (0, 0) -- (1, 1) -- (5, 1) -- (8, 4) -- (15, 4) -- (18, 7) -- (20, 7);
		\end{tikzpicture}    
  \caption{The workload function $\widehat{W}_j^0(\Delta, c_j^*)$ when $T_j=10$, $C_j=3$, and $R_j=6$. Solid line: $c_j^*$ is $3$, Dashed line: $c_j^*$ is $2$, Dotted line: $c_j^*$ is $1$.}
  \label{fig:example-lemma-workload}
\end{figure}

\noindent{\bf Physical Meaning of Theorem~\ref{theorem:general-framework}}



\begin{figure*}[t]
  \centering

		\def\ux{0.25cm}\def\uy{0.5cm} 
		\begin{tikzpicture}[y=\uy, font=\sffamily,thick]  
		\tikzset{
			task/.style={fill=#1,  rectangle, text height=.3cm},
			task1a/.style={task=green!30},
			task1b/.style={task=green},
			task2a/.style={task=orange!30},
			task2b/.style={task=orange, minimum width=1mm},
			task3a/.style={task=pink, minimum width=1mm},
			task3b/.style={task=pink!80, minimum width=1mm},
			task4a/.style={task=cyan, minimum width=1mm},
			task4b/.style={task=cyan!50},
			task5/.style={task=blue},
			task6/.style={task=purple},
			task7/.style={minimum height=\ux,draw},
			task8/.style={minimum height=\ux,draw,thick},
			task9/.style={task=gray,minimum height=0.7cm,draw},
		}
		\tikzstyle{jobs}=[ fill=black!50];

		\begin{scope}[shift={(0,0)}]
		
		\draw[<-](2,1.1) -- (2,0);
		\draw[<-](5,1.1) -- (5,0);
		\draw[<-](8,1.1) -- (8,0);
		\draw[<-](11,1.1) -- (11,0);
		
		
		\draw[->] (0,0) node[anchor=east] {$\tau_1$}-- coordinate (xaxis) (12.5,0);

		
		\node[task7, minimum width=2*\ux,
		anchor=south west]at (3, 0){};
		\node[task7, minimum width=2*\ux,
		anchor=south west]at (5, 0){};
		\node[task7, minimum width=2*\ux,
		anchor=south west]at (8, 0){};
		\node[task7, minimum width=2*\ux,
		anchor=south west]at (11, 0){};
		\draw[(-), thin] (2, 1.5) -- (3, 1.5);
		\node[anchor=south] at (2.5, 1.5) {$\sum_{i=1}^{k-1} x_i \cdot S_i=2$};

		\end{scope}
		
		
		\begin{scope}[shift={(0,-1.5)}]
		%%timeline
		
		
		\draw[<-](2.5,1.1) -- (2.5,0);
		\draw[<-](3,1.1) -- (3,0);
		\draw[<-](8,1.1) -- (8,0);
		
		
		\node[task7, minimum width=2*\ux,
		anchor=south west] at (3.5, 0){};
		\node[task7, minimum width=2*\ux,
		anchor=south west] at (4, 0){};
		\node[task7, minimum width=2*\ux,
		anchor=south west] at (8.5, 0){};

		\draw[->] (0,0)node[anchor=east] {$\tau_2$} -- coordinate (xaxis) (12.5,0);
		
		
		\end{scope}

		\begin{scope}[shift={(0,-3)}]
		%%timeline
		
		\draw[<-](2.5,1.1) -- (2.5,0);
		\draw[<-](11.5,1.1) -- (11.5,0);
		

		\draw[->] (0,0)node[anchor=east] {$\tau_3$} -- coordinate (xaxis) (12.5,0);
		
		\node[task7, minimum width=2*\ux,
		anchor=south west]  at ( 4.5, 0){};
		\node[task7, minimum width=6*\ux,
		anchor=south west]  at ( 5.5, 0){};
		
		\end{scope}

		\begin{scope}[shift={(0,-4.5)}]
		\draw[->] (0,0) node[anchor=east] {$\tau_4$}-- coordinate (xaxis) (12.5,0);
		\draw[<-](3.5,1.1) -- (3.5,0)node[anchor=north] {$$};
				\node[task7, minimum width=4*\ux,anchor=south west]at (7, 0){};
		\node[task7, minimum width=6*\ux,anchor=south west]at (9, 0){};
		
		\foreach \x in {0,...,24}{
			\draw[-](.5*\x,0.1) -- (.5*\x,-0.1)
			node[below] {\small $\x$};
			
		}

%                \draw (10, -1.5) node {\small $f_4$};
                \draw (3, -1.5) node {\small $t_4^*$};
 %               \draw (3.5, -1.5) node {\small $t_4$};
                \draw (2.5, -1.5) node {\small $t_3^*$};
                \draw (2.5, -2.2) node {\small $t_2^*$};
                \draw (2, -1.5) node {\small $t_1^*$};
		\end{scope}
		
		\end{tikzpicture}  

\caption{An illustrative example for  the physical meaning of
  Theorem~\ref{theorem:general-framework} for Example~\ref{ex:proof_step1}.}
\label{fig:example-imaginary}  
\end{figure*}
The rationale behind Theorem \ref{theorem:general-framework} may not
be easy to be captured. A specific vector $\vec{x}$ defines how
we plan to set the release jitter for each task as follows:
\begin{itemize}
\item For task $\tau_{k-1}$, its release jitter is $R_{k-1}-C_{k-1}$
  if $x_{k-1}$ is $0$ or $S_{k-1}$ if $x_{k-1}$ is $1$.
\item For task $\tau_j$ with $j=1,2,\ldots,k-2$, its release jitter
  is
  $Q_{j+1}^{\vec{x}} + R_j-C_j$ if $x_j$ is $0$ or $Q_{j+1}^{\vec{x}}
  + S_j$ if $x_j$ is $1$.
\end{itemize}
We use the following example to explain the physical meaning
behind the setting of the release jitter of the tasks by referring to
Step 3 in the proof of Theorem~\ref{theorem:general-framework}.

  We consider Example~\ref{ex:proof_step1} when $\epsilon$ is very
  close to $0$. For such a case, $t_1^*=4,
  t_2^*=5, t_3^*=5$, and $t_4^*=6$. We consider $R_2=10$. By the above
  setting with $x_1=1, x_2=0, x_3=1$, we know that
  \begin{compactitem}
  \item the release jitter of task $\tau_3$ is $1$ with the first
    release at time $t_4^*=6$,
  \item the release jitter of task $\tau_2$ is $1+10-1=10$ with the first
    release at time $t_4^*=6$, and
  \item the release jitter of task $\tau_1$ is $1+1=2$ with the first
    release at time $t_4^*=6$.
  \end{compactitem}
  Or alternatively, we can equivalently rephrase it as follows:
  \begin{compactitem}
  \item the release jitter of task $\tau_3$ is $0$ with the first
    release at time $t_3^*=5$,
  \item the release jitter of task $\tau_2$ is $10-1=9$ with the first
    release at time $t_2^*=5$, and
  \item the release jitter of task $\tau_1$ is $0$ with the first
    release at time $t_1^*=4$.
  \end{compactitem}


  Therefore, the response time analysis in
  Lemmas~\ref{lemma:step-3-one-condition}~and~\ref{lemma:step-3-ceiling-condition}
  can be explained as follows:

  \begin{quote}\emph{A safe scenario to analyze the worst-case
      response time $R_k'$ of task $\tau_k'$ when $R_k' \leq T_k$ is
      1) to release each higher-priority task $\tau_j$ at time
      $t_j^*\equals \sum_{i=1}^{j-1} x_j S_j$ with release jitter
      $(1-x_j)(R_j-C_j)$, and 2) to execute the accumulated work only
      after time $t_k^*$, where $t_1^*$ is an arbitrary constant.}
  \end{quote}

  Figure~\ref{fig:example-imaginary} provides a schedule based on the
  above setting. Note that self-suspension does not have to be
  accounted any more after the above transformation.  Task $\tau_1$ is
  an ordinary periodic task with period $6$ with the first release at
  time $4$, and task $\tau_3$ is an ordinary periodic task with period
  $18$ with the first release at time $5$. Task $\tau_2$ is a jittered
  periodic task with period $10$ and $9$ time-unit jitter, starting at
  time $5$. Therefore, the second job of task $\tau_2$ is released at
  time $6$ in Figure~\ref{fig:example-imaginary}.

  The two idle time units are used between time $4$ and time
  $6$. These two time units are \emph{blocked} simply for accounting
  the self-suspension behavior in ${\bf T}_1$, and no job is allowed
  to be executed in this time frame.  The accumulated workload is then
  started to be executed at time $6$ and the processor does not idle
  after time $6$. Over here, we see that two jobs of task $\tau_2$ are
  executed back to back from time 7 to time 9. As shown in Figure~\ref{fig:example-imaginary}, the
  processor is busy executing the workload from time $6$ to time
  $21$. Therefore, we know that $21-6=15$ is a safe upper bound of
  $R_4$ in this example.

%The conclusion we have in the final statement of the example is that $20-7=f_k-r_k \leq  f_k-6 < 21-6$.



\begin{appProof}{of Lemma~\ref{lem:dominance_blocking}}
  In this proof, we first transform the worst-case response time analysis presented in Corollary~\ref{corollary:general-framework} in a more pessimistic analysis. We then prove that this more pessimistic version of Corollary~\ref{corollary:general-framework} provides the same solution as Eq.~\eqref{eq:TDA-suspension}, which then proves the lemma.
  
  Since $Q_i^{\vec{x}} \equals \sum_{j=i}^{k-1} S_j \times x_j$, it holds that $Q_i^{\vec{x}} \leq  Q_1^{\vec{x}}$ for $i=1,2,\ldots,k-1$. It follows that
  \begin{align*}
  & C_k + S_k + \sum_{i=1}^{k-1}\ceiling{\frac{t+Q_i^{\vec{x}}+(1-x_i)(R_i-C_i)}{T_i}} C_i\\
  \stackrel{(Q_i^{\vec{x}} \leq  Q_1^{\vec{x}})}{\leq} & C_k + S_k + \sum_{i=1}^{k-1}\ceiling{\frac{t+Q_1^{\vec{x}}+(1-x_i)(R_i-C_i)}{T_i}} C_i\\
  \stackrel{(R_i \leq D_i \leq T_i)}{\leq} & C_k + S_k + \sum_{i=1}^{k-1}\ceiling{\frac{t+Q_1^{\vec{x}}+(1-x_i)T_i}{T_i}} C_i\\
  \stackrel{(x_i \in \{0,1\})}{=} & C_k + S_k + \sum_{i=1}^{k-1} (1-x_i)C_i + \sum_{i=1}^{k-1}\ceiling{\frac{t+Q_1^{\vec{x}}}{T_i}} C_i
  \end{align*}
  
  Therefore, the smallest positive value $t$ such that  
  \begin{equation}
  \label{eq:proof_dominance_blocking}
  C_k + S_k + \sum_{i=1}^{k-1} (1-x_i)C_i + \sum_{i=1}^{k-1}\ceiling{\frac{t+Q_1^{\vec{x}}}{T_i}} C_i\leq t
  \end{equation}
  is always larger than or equal to the solution of Eq.~\eqref{eq:TDA-suspension-tighter}. 
  
  Subtituting $(t+Q_1^{\vec{x}})$ by $\theta$ in Eq.~\eqref{eq:proof_dominance_blocking}, we get that $R_k$ is upper bounded by the minimum value $(\theta-Q_1^{\vec{x}})$ greater than $0$ (and therefore by the smallest $\theta > 0$) such that 
  \begin{align}
  & C_k + S_k + \sum_{i=1}^{k-1} (1-x_i)C_i + \sum_{i=1}^{k-1}\ceiling{\frac{\theta}{T_i}}C_i\leq \theta-Q_1^{\vec{x}} \nonumber \\
  \Leftrightarrow \;\; & C_k + S_k + Q_1^{\vec{x}} + \sum_{i=1}^{k-1} (1-x_i)C_i + \sum_{i=1}^{k-1}\ceiling{\frac{\theta}{T_i}}C_i\leq \theta \nonumber \\
\Leftrightarrow \;\; & C_k + S_k + \sum_{i=1}^{k-1}(x_i S_i + (1-x_i) C_i) + \sum_{i=1}^{k-1}\ceiling{\frac{\theta}{T_i}}C_i \leq \theta \label{eq:proof_dominance_blocking_final}.
    \end{align}
    
    
    Now, consider the particular vector assignment $\vec{x}$ in which 
  \begin{equation*}
    x_i =
    \begin{cases}
      1 &\mbox{if } S_i \leq    C_i\\
      0 & \mbox{otherwise},
    \end{cases}
  \end{equation*}
  for $i=1,2,\ldots,k-1$. 
    By the definition of $B_k$ (i.e., Section~\ref{sec:suspension-blocking}), we get that 
    $$B_k = S_k + \sum_{i=1}^{k-1} \min(C_i, S_i) = S_k + \sum_{i=1}^{k-1} \left(x_i
    S_i + (1-x_i) C_i\right)$$
Eq.~\eqref{eq:proof_dominance_blocking_final} thus becomes identical to Eq.~\eqref{eq:TDA-suspension}. Therefore, if Eq.~\eqref{eq:TDA-suspension} deems a task set as being schedulable, so does Corollary~\ref{corollary:general-framework}. 
\end{appProof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master.tex"
%%% End:
