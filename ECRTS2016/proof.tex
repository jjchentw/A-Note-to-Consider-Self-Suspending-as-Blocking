\subsection{Proof of Correctness}  
\label{sec:proof-th1}

We now provide the proof to support the correctness of the response time analysis presented in
Theorem~\ref{theorem:general-framework}, whatever the binary values
used in vector $\vec{x}$.  %The proof strategy is to
%show that the worst-case response time of task $\tau_k$ can be safely
%upper-bounded by any assignment of $\vec{x}$ of the $k-1$
%higher-priority tasks when adopting
%Eq.~(\ref{eq:TDA-suspension-tighter0}) as the response time analysis.
Throughout the proof, we consider any arbitrary assignment
$\vec{x}$, in which $x_i$ is either $0$ or $1$. For the sake of clarity, we classify the $k-1$
higher-priority tasks into two sets: ${\bf T}_0$ and ${\bf T}_1$. A
task $\tau_i$ is in ${\bf T}_0$ if $x_i$ is $0$; otherwise, it is in
${\bf T}_1$.
Our analysis is also based on very simple properties and lemmas enunciated as follows:

\begin{Property}
\label{prop:lower-priority}
In a preemptive fixed-priority schedule, the lower-priority jobs do not impact the schedule of the higher-priority jobs.
\end{Property}

%\begin{lemma}
%\label{lemma:remove-lower-priority}
%In a preemptive fixed-priority schedule, removing a lower-priority job arrived at time $t$ does not affect the schedule of the higher-priority jobs after time $t$.
%\end{lemma}
%\begin{proof}
%This is a direct consequence of Property~\ref{prop:lower-priority}.
%\end{proof}

\begin{Lemma}
\label{lemma:remove-same-task}
In a preemptive fixed-priority schedule, if the worst-case response time of task $\tau_i$ is no more than its period $T_i$, removing a job of task $\tau_i$ does not affect the schedule of any other jobs of task $\tau_i$.
\end{Lemma}
\begin{proof}
The proof is in Appendix\citetechreport{}.
\end{proof}


We now present the detailed proof of
Theorems~\ref{theorem:general-framework} and~\ref{theorem:general-framework-not-feasible} using the properties stated above. Since the proof is quite long, we 
will also provide examples to demonstrate the key steps in the proof and lemmas to support intermediate results.

%\begin{appProof}{Theorem~\ref{theorem:general-framework}}

% As already discussed in Section~\ref{sec:existing-analyses}, one can greedily convert the suspension time of task $\tau_k$ in computation time. Let $\tau_k'$ be this converted version of task $\tau_k$, i.e., $\tau_k' = (C_k + S_k, 0, D_k, T_k)$.  Suppose that $R_k'$ is the worst-case response time of $\tau_k'$ in the modified task set $\setof{\tau_1, \tau_2, \ldots, \tau_{k-1}, \tau_k'}$. It was already shown in previous works, e.g., Lemma~3 in
% \cite{Liu_2014}, that $R_k'$ is a safe upper bound on the worst-case response time of task $\tau_k$ in the original task set.

% Consider the modified task set $\tau'$ composed of $\left\{\tau_1, \tau_2, \ldots, \tau_{k-1}, \tau_k', \tau_{k+1}, \ldots \right\}$ where $\tau_k' = (C_k + S_k, 0, D_k, T_k)$. 

Let $\Psi$ be a fixed-priority preemptive schedule of the task system $\tau$.
Suppose that a job $J_{k}$ of task $\tau_k$ arrives at time $r_k$ and finishes at time $f_k$. 
By the assumption of $R_k \leq T_k$, we have $f_k \leq r_k+R_k \leq r_k+T_k$.
We first prove that Eq.~\eqref{eq:TDA-suspension-tighter0} gives us a
safe upper bound on $f_k-r_k$ for any job $J_k$ in $\Psi$ if $R_k \leq
T_k$.
% and Theorem~\ref{theorem:general-framework-not-feasible} is respected if $R_k > T_k$.
The proof is built upon the three following steps:
\begin{enumerate}
\item We discard all the jobs that arrive before $r_k$ and do not contribute to the response time of $J_k$ in the schedule $\Psi$. We follow an inductive strategy by iteratively inspecting the schedule of the higher priority tasks in $\Psi$, starting with $\tau_{k-1}$ until the highest priority task $\tau_1$. At each iteration, a time instant $t_j$ is identified such that $t_j \leq t_{j+1}$ ($1 \leq j < k$). Then, all the jobs of task $\tau_j$ released before $t_j$ are removed from the schedule and, if needed, replaced by an artificial job mimicking the interference caused by the residual workload of task $\tau_j$ at time $t_j$. %on the response time of job $J_k$. 
\item The final reduced schedule is analyzed to characterize important properties of the reduced schedule in Step 1.
\item We then prove that the response time analysis in Eq.~(\ref{eq:TDA-suspension-tighter0}) is indeed an upper bound on the worst-case response time $R_k$ of $\tau_k$.
\end{enumerate}

%!!!!!*************!!!!!!!!\\
%Since the proof involves several transformation steps. To illustrate some important steps in
%the proof, we also provide one concrete example. Consider a task
%system with 
%\begin{itemize}
%\item $T_1 = 6, C_1 = 1, S_1 = 1, x_1=1$,
%\item $T_2 = 10, C_2 = 1, S_2 = 6, x_2=0$,
%\item $T_3 = 18, C_3 = 4, S_3 = 1, x_3=1$,
%\item $T_4 = 20, C_4 = 5, S_4 = 0$.
%\end{itemize}
  
%!!!!!*************!!!!!!!!


\begin{figure*}[t]
  \centering
  \subfloat[$\Psi$, $\Psi^4$ and $\Psi^3$]{
    \scalebox{0.65}{
      \input{example-step1-body.tex}}}
  \subfloat[$\Psi^2$ and $\Psi^1$]{
    \scalebox{0.65}{
      \input{example-step1-body-v2.tex}}}
\caption{An illustrative example of Step 1 in the proof of Theorem~\ref{theorem:general-framework} when $\epsilon = 0.1$.}
\label{fig:example}  
\end{figure*}


\noindent{\bf Step 1: Reducing the schedule $\Psi$} 

\emph{Our purpose in this step is to discard all the jobs that arrive before $r_k$ and have no impact on the response time of $J_k$ in the schedule
  $\Psi$. } During this step, we iteratively build the schedules from $\Psi^k$ to $\Psi^1$ mentioned above. Based on a given schedule $\Psi^{j+1}$ (with $1 \leq j < k$), we build the fixed-priority schedule $\Psi^j$ such that the response time of $J_k$ remains identical. At each iteration, we define $t_j$ for task $\tau_j$ in the schedule $\Psi^{j+1}$ and build $\Psi^j$ by removing all the jobs released by $\tau_j$ before $t_j$. \emph{We then prove that the response time of $J_k$ in the reduced fixed-priority schedule $\Psi^1$ remains the same as the response time of $J_k$ in  the original fixed-priority schedule $\Psi$.}


\noindent\textit{Basic step (definition of $\Psi^k$ and $t_k$):} 

%Recall that the job $J_{k}$ of task $\tau_k$ arrives at time $r_k$ and finishes at time $f_k$ in schedule $\Psi$.  
We define $\Psi^k$ as the schedule in which (i) all high-priority tasks $\tau_1, \ldots, \tau_{k-1}$ release their jobs at the exact same instants as in $\Psi$, (ii) $\tau_k$ releases only one job at time $r_k$, (iii) the low-priority tasks $\tau_{k+1}, \ldots, \tau_n$ do not release any job, and (iv) all jobs suspend their execution after the exact same execution time as in $\Psi$. Moreover, since $J_k$ is released at time $r_k$ and does not finish strictly before $f_k$, the total amount of idle time of the system from $r_k$ to $f_k$ is at most $S_k$. In the converted schedule $\Psi^k$, we further convert the idle time as part of the execution time of $J_k$. After the conversion by considering suspension as computation for job $J_k$, we know that the worst-case execution time of $J_k$ is upper bounded by $C_k'=S_k+C_k$.  As already discussed in Section~\ref{sec:existing-analyses}, such a conversion has been widely used. For notational brevity, we denote this job $J_k$ as a release of task $\tau_k' = (C_k + S_k, 0, D_k, T_k)$. It is obvious that $\Psi^k$ remains as a fixed-priority schedule.

\begin{Lemma}
\label{lemma:psik}
The response time of $J_k$ in $\Psi^k$ is the same as the response time of $J_k$ in $\Psi$.
\end{Lemma}
\begin{proof}
We know by Property~\ref{prop:lower-priority} that the lower priority tasks $\tau_{k+1}, \tau_{k+2}, \ldots, \tau_n$ do not impact the response time of $J_{k}$. Therefore, not releasing them has no impact on the response time $J_k$. Moreover, since we assume that the worst-case response time of task $\tau_k$ is no more than $T_k$, Lemma~\ref{lemma:remove-same-task} proves that none of the jobs of task $\tau_k$ except $J_{k}$ impacts the schedule of $J_{k}$. Since all the other parameters (i.e., releases and suspensions) that may influence the scheduling decisions are kept identical between $\Psi$ and $\Psi^k$, the response time of $J_{k}$ in $\Psi^{k}$ is identical to the response time of $J_{k}$ in $\Psi$.
\end{proof}

To allow the induction defined below, we also define $t_k$ as the release time of $J_k$ (i.e., $t_k \equals r_k$).

\noindent\textit{Induction step (definition of $\Psi^j$ and $t_j$ with $1 \leq j < k$):}

%Let $r_j$ be the arrival time of the last job released by $\tau_j$ before $t_{j+1}$ in $\Psi^{j+1}$ and let $J_{j}$ denote that job. By definition, $r_j < t_{j+1}$. %There are a two possible cases:
%\begin{itemize}
%\item $J_{j}$ completed its execution no later than $t_{j+1}$. Then, we simply set $t_j$ to $t_{j+1}$ and generate $\Psi^j$ by removing all the jobs of task $\tau_j$ arrived before $t_{j+1}$ in the schedule $\Psi^{j+1}$. By Lemma~\ref{lemma:remove-same-task} and Property \ref{prop:lower-priority}, removing all the jobs of task $\tau_j$ arrived before $t_{j+1}$ has no impact on the schedule of the higher-priority jobs (jobs released by $\tau_1, \ldots, \tau_{j-1}$) and the jobs of $\tau_j$ released after $t_{j+1}$. Moreover, because no task with a priority lower than $\tau_j$ executes jobs before $t_{j+1}$ in $\Psi^{j+1}$, removing the jobs released by $\tau_j$ before $t_{j+1}$ does not impact the schedule of the jobs of $\tau_{j+1}, \ldots, \tau_{k}$. The response time of $J_{k}$ in $\Psi^j$ thus remains unchanged in comparison to its response time in $\Psi^{j+1}$. 
%\item $J_{j}$ did not complete its execution before $t_{j+1}$.

We define four cases in order to build $\Psi^j$ from $\Psi^{j+1}$.

\noindent{\bf Case 0.} If all the jobs of task $\tau_j$ are released at or after $t_{j+1}$ in schedule $\Psi^{j+1}$, then we define $\Psi^j$ as being identical to $\Psi^{j+1}$ and set $t_j \equals t_{j+1}$. 

%If {\bf Rule 0} is applied than $\Psi^j$ and $\Psi^{j+1}$ are identical and the response time of $J_k$ is then unchanged. 

Now, let us consider that  task $\tau_j$ releases at least one job before $t_{j+1}$ in $\Psi^{j+1}$. Let $r_j$ be the arrival time of the last job released by $\tau_j$ before $t_{j+1}$ in $\Psi^{j+1}$ and let $J_{j}$ denote that job. By definition, $r_j < t_{j+1}$. Let $c_j^*$ be the remaining execution time of $J_j$ at time $t_{j+1}$ in $\Psi^{j+1}$. By definition, $0 \leq c_j^* \le C_j$. In the rest of the proof, $c_j^*$ is called $\tau_j$'s \emph{residual workload}.

We start by setting $\Psi^j$ to be identical as $\Psi^{j+1}$. Then, all the jobs of task $\tau_j$ that arrive before $r_j$ are immediately removed from $\Psi^j$. That is, all jobs released in $\Psi^j$ have identical suspension and execution behavior as in $\Psi^{j+1}$, and task $\tau_j$ does not release any job before $r_j$ in $\Psi^j$. 
There are three cases to decide how we include or exclude $J_j$ in $\Psi^j$ as follows:


\noindent{\bf Case 1.} If $\tau_j \in {\bf T}_1$ and $J_{j}$ does not complete its execution by $t_{j+1}$ in the schedule $\Psi^{j+1}$ (i.e., $c_j^*>0$), then $t_j \equals r_j$ and $J_j$ is included in $\Psi^j$. In this case, task $\tau_j$ releases its jobs at exactly the same instants in $\Psi^{j+1}$, i.e., \emph{at and after} $r_j$.

\noindent{\bf Case 2.} If $\tau_j \in {\bf T}_1$ and $J_{j}$ completes its execution before or at $t_{j+1}$ in the schedule $\Psi^{j+1}$ (i.e., $c_j^* = 0$), then $t_j \equals t_{j+1}$ and $J_j$ is further removed and excluded from $\Psi^j$. In this case, task 
 $\tau_j$ releases its jobs at exactly the same instants in $\Psi^{j+1}$ \emph{after} $r_j$.

\noindent{\bf Case 3.} If $\tau_j \in {\bf T}_0$, then $t_j \equals t_{j+1}$ and (i) $\tau_j$ releases its jobs at the same instants in $\Psi^{j+1}$ \emph{after} $r_j$ (i.e., exclude $J_j$ in $\Psi^j$), and (ii) an artificial (or additional) job $J_a$ with execution time $C_a \equals c_j^*$ and the same priority as $\tau_j$ is released at time $t_{j+1}$. This artificial job follows the same execution and suspension behavior as job $J_j$ after $t_{j+1}$.

After the above procedures, it is obvious that the resulting schedule $\Psi^j$ remains as a fixed-priority schedule.
\begin{Lemma}
\label{lemma:psij}
The response time of $J_k$ in $\Psi^j$ is the same as the response time of $J_k$ in $\Psi^{j+1}$.
\end{Lemma}
\begin{proof}
If Case 0 is applied, then $\Psi^j$ and $\Psi^{j+1}$ are identical. The response time of $J_k$ is thus unchanged and the claim trivially holds.

For the rest of the proof, we use the four following facts:

\noindent{\bf Fact 1.} For any $\ell$ such that $j \leq \ell < k$, there is $t_\ell \leq t_{\ell+1}$ and $\tau_\ell$ does not release any job before $t_\ell$ in $\Psi^j$.

\noindent{\bf Fact 2.} No job of $\tau_k, \ldots, \tau_n$ are released before $t_k$ in $\Psi^k$.

\noindent{\bf Fact 3.} By the assumption that $R_j \leq D_j \leq T_j$ for $j=1,2,\ldots,k-1$, removing all the jobs of task $\tau_j$ arrived before $r_j$ has no impact on the schedule of any other job released by $\tau_j$ (Lemma~\ref{lemma:remove-same-task}) or any higher priority job released by $\tau_1, \ldots, \tau_{j-1}$ (Property \ref{prop:lower-priority}). Moreover, by Facts 1 and 2, no task with a priority lower than $\tau_j$ (tasks $\tau_{j+1}, \ldots, \tau_n$) release jobs before $t_{j+1}$ in $\Psi^{j+1}$. Therefore, removing the jobs released and completed by $\tau_j$ before $r_j$ does not impact the schedule of the jobs of $\tau_{j+1}, \ldots, \tau_{n}$. \emph{Consequently, we can safely remove all the jobs of task $\tau_j$ arrived and completed before $t_{j+1}$ without impacting the response time of $J_{k}$.}

\noindent{\bf Fact 4.} Since by Facts 1 and 2, no task with a priority lower than $\tau_j$ (tasks $\tau_{j+1}, \ldots, \tau_n$) releases jobs before $t_{j+1}$ in $\Psi^{j+1}$, \emph{replacing $J_j$ with the created artificial job (which has the same execution and suspension behavior as $J_j$ from $t_{j+1}$) has no impact on the schedule of $\tau_{j+1}, \ldots, \tau_n$ in $\Psi^{j+1}$.}

%But, should we include or exclude $J_j$ for the construction of
%$\Psi^j$?  We use three rules to define the construction of the
%schedule $\Psi^j$ and $t_j$ for including or excluding $J_j$.
%\begin{compactitem}
%\item {\bf Rule 1 assigns $t_j$ to $r_j$ and includes $J_j$:} This rule is applied to
%  include the complete job of task $\tau_j$ released at time $r_j$
%  in the schedule $\Psi^j$. 
%\item {\bf Rule 2 assigns $t_j$ to $t_{j+1}$ and excludes $J_j$:} This rule is applied to
%  completely exclude $J_j$ in the schedule $\Psi^j$. 
%\item {\bf Rule 3 assigns $t_j$ to $t_{j+1}$ and partially excludes
%    $J_j$:} This rule is applied to partially exclude $J_j$ in the
%  schedule $\Psi^j$.  If we apply this rule, we will remove $J_j$ and
%  create an artificial job to represent the residual workload of $J_j$
%  (to be explained and defined later), executed at or after $t_{j+1}$.
%\end{compactitem}

We now consider the three remaining cases:

\noindent In Case 1, $\Psi^j$ is built from $\Psi^{j+1}$ by removing all the jobs released by $\tau_j$ \emph{before} $r_j$.  All the excluded jobs have therefore completed their execution at $t_{j+1}$ and by Fact 3, this has no impact on the execution of any job executed after $t_{j+1}$ and thus on the response time of $J_k$.

\noindent In Case 2, $\Psi^j$ is built from $\Psi^{j+1}$ by removing all the jobs released by $\tau_j$ before $t_{j+1}$. Since $J_{j}$ completes before $t_{j+1}$, by Fact 3, none of the excluded jobs impacted the response time of $J_k$. \emph{The response time of $J_{k}$ in $\Psi^j$ thus remains unchanged in comparison to its response time in $\Psi^{j+1}$.}

\noindent In Case 3, all the jobs of $\tau_j$ released before $t_{j+1}$ are removed and the job of task $\tau_j$ arrived at time $r_j$ is replaced by a new job released at time $t_{j+1}$ with execution time $c_j^*$ and the same priority as $\tau_j$. \emph{By Facts 3 and 4, the interference generated by $\tau_j$ and the additional job on job released at or after $t_{j+1}$ remains identical between $\Psi^j$ and $\Psi^{j+1}$. Thus, the response time of $J_k$ is unchanged.} 
\end{proof}

 
\noindent\textit{Conclusion of Step 1}:

This iterative process is repeated until producing $\Psi^1$. The procedures are well-defined and it is therefore guaranteed that $\Psi^1$ can be constructed. A pseudo-code of $\Psi^1$'s generation procedure can be found in Appendix\citetechreport{}. Note that after each iteration, the number of jobs considered in the resulting schedule has been reduced, yet without affecting the response time of $J_k$, as proven in the following lemma.

 \begin{Lemma}
\label{lem:step-1-conclusion}
 The response time of job $J_k$ in $\Psi^1$ is the same as the response time of $J_k$ in $\Psi$.
\end{Lemma}
\begin{proof}
  By Lemma~\ref{lemma:psik}, the reponse time of $\tau_k$ in $\Psi^k$ is identical to the response time of $J_k$ in $\Psi$. And by inductively applying Lemma~\ref{lemma:psij}, we get that the response time of $\tau_k$ in $\Psi^k$ is identical to the response time of $\tau_k$ in $\Psi^{k-1}$, $\Psi^{k-2}$, \ldots, $\Psi^{1}$. This proves the lemma.
\end{proof}


\begin{example}
\label{ex:proof_step1}
Consider 4 tasks $\tau_1 =(1,1,6,6)$, $\tau_2 =(1,6,10,10)$, $\tau_3 =(4,1,18,18)$ and $\tau_4 =(5,0,20,20)$.
We assume $x_1=1$, $x_2=0$ and $x_3=1$.
Figure~\ref{fig:example}(a) depicts a possible schedule $\Psi^4$ of those tasks. We assume that the first job of task $\tau_1$ arrives
at time $4+\epsilon$ with $0 < \epsilon < 0.5$. The first job
of task $\tau_2$ suspends itself from time $0$ to time $5+\epsilon$,
and is blocked by task $\tau_1$ from time $5+\epsilon$ to time
$6+\epsilon$. After executing $\epsilon$ amount
of time, the first job of task $\tau_2$ suspends itself again from
time $6+2\epsilon$ to $7$. The schedule in Figure~\ref{fig:example}(a) is drawn for $\epsilon=0.1$.
   
In the schedule illustrated in Figure~\ref{fig:example}(a), $f_4$ is
$20-\epsilon$.  Based on the definition of $t_k$, $t_4=7$. Then, we set $t_3$ to $6$ by applying Case 1. The
schedule $\Psi^3$ is identical to the original schedule $\Psi^4$. When considering task $\tau_2$, we know that $J_2$ is the job of task
$\tau_2$ arrived at time $r_2 = 0 < t_3$. Since task $\tau_2$ belongs to ${\bf
  T}_0$, by applying Case 3, we have $t_2=t_3=6$ and the
residual workload $c_2^*$ is $1$. Then, we remove job $J_2$ from the
schedule and create an artificial job with execution time $c_2^*$ that
is released at time $t_2$ and assign the artificial job the same priority level as task
$\tau_2$. Note that this artificial job can still suspend itself. Therefore, the schedule $\Psi^2$, as drawn in 
Figure~\ref{fig:example}(b), is slightly different from $\Psi^3$, shown in Figure~\ref{fig:example}(a). Yet, the response time of $J_4$ is unchanged. Finally, $t_1$ is set to $4+\epsilon$ by applying Case 1 since
$J_1$ (arrived at time $r_1=4+\epsilon$) has not completed yet at time
$t_2=6$.  The schedule $\Psi^1$ is identical to the schedule
$\Psi^2$.
% We have to further consider one job of task $\tau_2$ arrived
% before time $t_1$ with execution time $C_2$. 
\myendproof
\end{example}

\noindent{\bf Step 2: Analyzing the reduced schedule $\Psi^1$}

We now analyze the properties of the final fixed-priority schedule $\Psi^1$ in which
all the unnecessary jobs have been removed. %Remember that by Lemma~\ref{lem:step-1-conclusion}, the response time of job $J_k$ is the same in both schedules $\Psi$ and $\Psi^1$. 
%The main goal of this
%step is to prove the correctness of the conditions listed in
%Lemma~\ref{lemma:conclusion-step2} for the schedule $\Psi^1$. 
This step is based on the simple fact that for any interval $[t_1, t)$ with $t \leq f_k$, there is 
\begin{equation}
\label{eq:exec_plus_idle}
\operatorname{idle}(t_1, t) + \operatorname{exec}(t_1, t)  = (t - t_1)
\end{equation}
where $\operatorname{exec}(t_1, t)$ is the amount of time during which the processor executes tasks within $[t_1, t)$, and $\operatorname{idle}(t_1, t)$ is the amount of time during which the processor remains idle within the interval $[t_1, t)$.

We first provide an upper bound on $\operatorname{idle}(t_1, t)$ (see Lemma~\ref{lem:max_idle} and Corollary~\ref{lem:bound_idle}), then on $\operatorname{exec}(t_1, t)$ (see Lemmas~\ref{lemma:exec-tau-k} to \ref{lemma:Wj0-dominate}). Finally, in Lemma~\ref{lemma:conclusion-step2}, we combine those results with Eq. \eqref{eq:exec_plus_idle} in order to characterise the schedule $\Psi^1$ in $[t_1, f_k)$.

%If $t_i < t_{i+1}$, the processor may idle in the time interval $[t_i,
%t_{i+1})$ in $\Psi^1$. 
%Let $\sigma_i$ be the total idle time in $\Psi^1$ within the time interval $[t_i, t_{i+1})$. 
We start our analysis with $\operatorname{idle}(t_1, t)$ when $t_1 < t \leq f_k$.
Let $\sigma_j$ be the amount of time during which the processor remains idle within $[t_j, t_{j+1})$ in $\Psi^1$. 
\begin{Lemma}
\label{lem:max_idle}
For $j=1,2,\ldots,k-1$, $\sigma_j  = x_j \times \sigma_j \leq x_j \times S_j$. %$\sigma_j \leq S_j$.
\end{Lemma}
\begin{proof}
If Case 1 is applied on $\tau_j$ when we build $\Psi^j$ in Step 1, (i) $x_j = 1$, (ii) $t_j$ is set to the release time $r_j$ of the job $J_j$, and (iii) $J_j$ has not completed its execution yet at time $t_{j+1}$. By (ii) and (iii), the
  amount of time during which the processor may remain idle within
  $[t_j, t_{j+1})$ is at most the suspension time $S_j$ of $\tau_j$. Thus, $\sigma_j \leq S_j$. And by (i), $\sigma_j = x_j \times \sigma_j \leq x_j \times S_j$.
  
  If Cases 0, 2 or 3 is applied on $\tau_j$ when we build $\Psi^j$ in Step 1, then $t_j$ is equal to $t_{j+1}$ and by definition, $\sigma_j= 0$. It results that $\sigma_j= x_j \times \sigma_j \leq x_j \times S_j$.  
  %Using the two previous equations, it results that
  %$\sum_{j=1}^{i-1} \sigma_j = \sum_{j=1}^{i-1} x_j \sigma_j \leq
  %\sum_{j=1}^{i-1} x_j S_j$.
\end{proof}

\begin{Corollary}
\label{lem:bound_idle}
For $i=1,2,\ldots,k-1$, $\forall t | t_{i} < t \leq t_{i+1}$, 
\begin{align}
\label{eq:sumof-sigma}
%\operatorname{idle}(t_1, t) \leq \sum_{i: t_i < t} \sigma_i
\operatorname{idle}(t_1, t) \leq \sum_{j=1}^{i} x_j \sigma_j \leq
  \sum_{j=1}^{i} x_j S_j
\end{align}
\end{Corollary}
\begin{proof}
Since $t_{i} < t \leq t_{i+1}$, $\operatorname{idle}(t_1, t) \leq \sum_{j=1}^{i} \sigma_j$. And by Lemma~\ref{lem:max_idle}, $\operatorname{idle}(t_1, t) \leq \sum_{j=1}^{i} x_j \sigma_j \leq \sum_{j=1}^{i} x_j S_j$
\end{proof}

\begin{example}
As shown in the schedule in Example~\ref{ex:proof_step1}, the total idle time from $4+\epsilon$ to
$20-\epsilon$, i.e., from $4+\epsilon$ to $5+\epsilon$ and from
$6+2\epsilon$ to $7$, is $2-2\epsilon$, which is upper-bounded by $S_1+S_3 = 2$. 
\end{example}

% by JJ 23,04,2016 -> this following step seems to be not useful. 
%
% Moreover, by the definition of $t_k$ and $f_k$, the schedule $\Psi^1$
% does not have any idle time between $t_k$ and $f_k$. Therefore, 
% it holds that $\forall t | t_k < t \leq f_k$, 
% \begin{align}
% \label{eq:sumof-sigma-2}
% %\operatorname{idle}(t_1, t) \leq \sum_{i: t_i < t} \sigma_i
% \operatorname{idle}(t_1, t) \leq \sum_{j=1}^{k-1} x_j S_j.
% \end{align}
% The above two cases can be found in the example in Figure~\ref{fig:example}.

%From Lemma~\ref{lem:max_idle}. %If $t_i$ is equal to $t_{i+1}$, then $\sigma_i$ is set to $0$. 

We now consider $\operatorname{exec}(t_1, t)$ when $t_1 < t \leq f_k$.
Because there is no job released by lower priority tasks than
$\tau_k'$ in $\Psi^1$, we only focus on the execution of the
tasks $(\tau_1, \tau_2, \ldots, \tau_{k-1}, \tau_k')$. Let
$\operatorname{exec}_j(t_1, t)$ be \emph{the
 (accumulative) amount of time that task $\tau_j$ is executed in the schedule
  $\Psi^1$ in the time interval $(t_1, t]$}. By the construction of the
schedule $\Psi^1$, we know that $\operatorname{exec}_j(t_1, t_j)$ must
be equal to $0$ since task $\tau_j$ is not executed between $t_1$ and $t_j$. %, as they were already discarded during the construction of $\Psi^j$.
Therefore, $\operatorname{exec}_j(t_1, t)$ is equal to
$\operatorname{exec}_j(t_j, t)$ if $t > t_j$.

\begin{Lemma}
  \label{lemma:exec-tau-k}
  $\forall t | t_k \leq t < f_k$, the (accumulative) amount of time that
  task $\tau_k'$ is executed from $t_k$ to $t$ is $\operatorname{exec}_k(t_k, t) < C_k'$.
\end{Lemma}
\begin{proof}
  Since the finishing time of job $J_k$ is at time $f_k$ in schedule $\Psi^1$, the
  condition holds by definition.
\end{proof}

\begin{Lemma}
\label{lemma:Wj1-exact}
If task $\tau_j \in {\bf T}_1$, then $\forall \Delta \geq 0$ we have 
\[
\operatorname{exec}_j(t_j, t_j+\Delta) \leq W_j^1(\Delta)
\]
where
{\footnotesize \begin{equation}
  \label{eq:execution-case1}
  W_j^1(\Delta) \stackrel{\mbox{def}}{=} \floor{\frac{\Delta}{T_j}}C_j + \min\left\{\Delta-\floor{\frac{\Delta}{T_j}}T_j, C_j\right\}.
\end{equation}}
\end{Lemma}
\begin{proof}
If task $\tau_j \in {\bf T}_1$, then Case 0, 1 or 2 is applied when building $\Psi^1$ in Step 1. % and the job $J_j$ has not finished yet at
  %time $t_{j+1}$. 
  In this case, $\Psi^1$ does not contain any job of task $\tau_j$
  arrived before $t_j$ (i.e., no residual workload of $\tau_j$ at time $t_j$). Furthermore, $\operatorname{exec}_j(t_j, t_j+\Delta)$ is maximized when the jobs released by $\tau_j$ after $t_j$ are actually executing, and hence do not suspend themselves (i.e., $\tau_j$ acts as a sporadic tasks without self-suspension). Since, as shown in the literature \cite{bertogna2006new}, $W_j^1(\Delta)$, which is usually called \emph{workload function}, is an upper bound on the amount of execution time that a sporadic task can execute without self-suspension, we know that $\operatorname{exec}_j(t_j, t_j+\Delta)$ of $\tau_j$ from $t_j$ to $t_j+\Delta$ is upper bounded by $W_j^1(\Delta)$.
%\item Task $\tau_j$ applies {\bf Rule 2}. This is the case when
%  $\tau_j \in {\bf T}_1$ and the job $J_j$ already finishes at time
%  $t_{j+1}$. In this case, it is clear that $\operatorname{exec}_j(t_j, t_j+\Delta)  \leq W_j^1(\Delta)$ also holds.
\end{proof}

\begin{Lemma}
\label{lemma:Wj0-exact}
If $\tau_j \in {\bf T}_0$, then $\forall \Delta \geq 0$ we have 
\[
\operatorname{exec}_j(t_j, t_j+\Delta) \leq \widehat{W}_j^0(\Delta, c_j^*) 
\]
where 
{\footnotesize \begin{equation}
    \label{eq:execution-case2-precise}
    \widehat{W}_j^0(\Delta, c_j^*)=
    \begin{cases}
      W_j^1(\Delta) & \mbox{ if } c_j^* = 0\\
      \Delta & \mbox{ if } c_j^* > 0 \mbox{ and } \Delta \leq  c_j^*\\
      c_j^* & \mbox{ if } c_j^* > 0 \mbox{ and } c_j^* < \Delta \leq  \rho_j\\
      c_j^* + W_j^1(\Delta-\rho_j) & \mbox{ if } c_j^* > 0 \mbox{ and } \rho_j < \Delta
    \end{cases}
  \end{equation}}
  and
  $\rho_j = (T_j-R_j +  c_j^*)$.
\end{Lemma}
\begin{proof}
If task $\tau_j \in {\bf T}_0$, then Case 0 or 3 is applied when building $\Psi^1$ in Step 1. %This is the case when
  %$\tau_j \in {\bf T}_0$. 
  Therefore, there might be a job $J_j$ arrived before
  $t_j$ with a residual workload $0 \leq c_j^* \leq C_j$ at time
  $t_j$. %For notational brevity, let $\rho_j$ be defined as $(T_j-R_j +
  %c_j^*)$. %We will show that $\operatorname{exec}_j(t_j, t_j+\Delta) \leq
%  \widehat{W}_j^0(\Delta, c_j^*)$, defined as follows for two cases for $\Delta \geq 0$:
%  If $c_j^*$ is $0$, then\footnote{The reader may think that this case
%    is not necessary, but it is, since the formulation in
%    Eq.~\eqref{eq:execution-case2-precise} does not cover this case.}
%  \begin{equation}
%  \label{eq:execution-case2-cj*=0}
% \widehat{W}_j^0(\Delta, 0)=    W_j^1(\Delta);
%  \end{equation}
%  otherwise if $c_j^* > 0$, then
%  \begin{equation}
%    \label{eq:execution-case2-precise}
%    \widehat{W}_j^0(\Delta, c_j^*)=
%    \begin{cases}
%      \Delta & \mbox{ if } \Delta \leq  c_j^*\\
%      c_j^* & \mbox{ if } c_j^* < \Delta \leq  \rho_j\\
%      c_j^* + W_j^1(\Delta-\rho_j) & \mbox{ otherwise}.
%    \end{cases}
%  \end{equation}
  The case when $c_j^*=0$ is identical to the proof of Lemma~\ref{lemma:Wj1-exact}. We now consider the cases where $c_j^* > 0$.
  %Two subcases are considered:
  %\begin{itemize}
  %\item If the residual workload $c_j^*$
  %of task $\tau_j$ is $0$, the earliest next arrival time of task $\tau_j$
  %can be any time instant at or after $t_j$. 
%In this case, for any $\Delta \geq 0$, the workload $\widehat{W}_j^0(\Delta, c_j^*)$ executed by $\tau_j$ from $t_j$ to $t_j+\Delta$ is upper bounded by $W_j^1(\Delta)$. 
%\item If the residual workload $c_j^*$ of task $\tau_j$ is larger than $0$,
  Since by assumption $R_j \leq D_j \leq T_j$, task 
  $\tau_j$ respects all its deadlines and the
  worst-case response time, the absolute deadline of the job $J_j$ of $\tau_j$ that is not completed yet at $t_j$, must be at least $t_j + c_j^*$. %otherwise that job would violate its worst-case execution time (that has been confirmed earlier). 
  Therefore, the earliest arrival time of a job of task $\tau_j$
  \emph{strictly after} $t_j$ is at least $t_j + c_j^* + (T_j-R_j) = t_j+\rho_j$. Since there is no other job of task $\tau_j$ released in $[t_j, \rho_j)$ except the artificial job with the residual workload $c_j^*$ created based on $J_j$, we know that 
$\operatorname{exec}_j(t_j, t_j+\Delta)$ is upper bounded by $\min\left\{ \Delta, c_j^* \right\}$ for $\Delta \leq \rho_j$, thereby proving cases 2 and 3 of Eq.~\eqref{eq:execution-case2-precise}. Furthermore, by assumption $J_j$ completes its execution before or at $t_j + \rho_j$. Therefore, following the same proof as Lemma~\ref{lemma:Wj1-exact}, $\operatorname{exec}_j(t_j + \rho_j, t_j + \Delta)$ is upper bounded by $W_j^1(\Delta -\rho_j)$ when $\Delta > \rho_j$. This proves the fourth case of Eq.~\eqref{eq:execution-case2-precise}.
\end{proof}

%\begin{Lemma}
%\label{lemma:Wj01-exact}
%$\forall \Delta \geq 0$ and given $c_j^*$ for a task $\tau_j \in {\bf T}_0$, we have 
%\[
%\begin{cases}
%\operatorname{exec}_j(t_j, t_j+\Delta) \leq \widehat{W}_j^0(\Delta, c_j^*) &\mbox{ if } \tau_j \in {\bf T}_0\\  
%\operatorname{exec}_j(t_j, t_j+\Delta) \leq W_j^1(\Delta) &\mbox{ if } \tau_j \in {\bf T}_1  
% \end{cases}
%\]
%\end{Lemma}
%\begin{proof}
%  Both cases have been discussed above.
%\end{proof}
For notational brevity, let $W_j^0(\Delta)\equals \widehat{W}_j^0(\Delta, C_j)$. We also prove that, for any $\Delta \geq 0$, $W_j^0(\Delta) \geq \widehat{W}_j^0(\Delta, c_j^*)$:
\begin{Lemma}
\label{lemma:Wj0-dominate}
$\forall \Delta \geq 0$,~ $W_j^0(\Delta) \geq \widehat{W}_j^0(\Delta, c_j^*)$.
%where 
%$\widehat{W}_j^0(\Delta, c_j^*)$ is given by Eq.~(\ref{eq:execution-case2-precise}).
\end{Lemma}
\begin{proof}
 The proof is based on simple observations of the workload function. The proof is in Appendix\ifbool{techreport}{.}{\citetechreport{}.}
\end{proof}

% When $c_j^* > 0$, it is easy to see that
% Eq.~\eqref{eq:execution-case2-precise} is maximized when $c_j^*$ is
% maximum, that is, when $c_j^* = C_j$. When $c_j^* = 0$, due to the assumption of constrained-deadline task sets, it is also easy to see that 
% $\widehat{W}_j^0(\Delta, 0) = W_j^1(\Delta) \leq \widehat{W}_j^0(\Delta, C_j)$
% It results that for all $\Delta \geq 0$ and $0 \leq c_j^* \leq C_j$, we have $\widehat{W}_j^0(\Delta, C_j) \geq
% \widehat{W}_j^0(\Delta, c_j^*)$.


%For the sake of notational brevity,
%we introduce the notation $W_j^0(\Delta)$ defined as follows.
%\begin{equation}
%  \label{eq:execution-case2-upperbounded}
% W_j^0(\Delta) \equals \widehat{W}_j^0(\Delta, C_j) 
%\end{equation}

%For the sake of notational brevity, for the rest of this proof, we use the notation $W_j^0(\Delta)$ to denote $\widehat{W}_j^0(\Delta, C_j)$.
Now that we have derived upper bounds on the idle time $\operatorname{idle}(t_1, t)$ and the execution time $\operatorname{exec}_j(t_j, t_j+\Delta)$ of each task $\tau_j$ executed in $\Psi^1$, we inject those results in Eq.~\eqref{eq:exec_plus_idle} in order to derive properties on the schedule in any interval $[t_1, t)$ for any $t_1 < t < f_k$.
\begin{Lemma}
  \label{lemma:conclusion-step2}
$\forall t \mid t_{i} \leq t < t_{i+1}$ where $i=1,2,\ldots,k-1$
{\footnotesize \begin{equation}
\label{eq:exec_plus_idle-2}
\sum_{j=1}^{i} \left( x_j\cdot (W_j^1(t-t_j) +\sigma_j) + (1-x_j)\cdot W_j^0(t-t_j) \right) \geq t-t_1.
\end{equation}}
 And, $\forall t \mid t_k \leq t < f_k$,
{\footnotesize \begin{equation}
\label{eq:exec_plus_idle-3}
C'_k +\sum_{j=1}^{k-1} \left( x_j \cdot (W_j^1(t-t_j) +\sigma_j) + (1-x_j)\cdot W_j^0(t-t_j) \right) > t-t_1.
\end{equation}}
\end{Lemma}
\begin{proof}
We combine the three following facts:

\noindent\textbf{1.} By Eq.~\eqref{eq:exec_plus_idle}, $\operatorname{idle}(t_1, t)+\operatorname{exec}(t_1, t) = t-t_1$.

\noindent\textbf{2.} By Corollary~\ref{lem:bound_idle}, $\operatorname{idle}(t_1, t) \leq \sum_{j=1}^{i-1}
x_j \sigma_j $ for all $t \mid t_{i} \leq t < t_{i+1}$ and $i=1,2,\ldots,k-1$.\footnote{\label{footnote-why-sigma}The readers
  may think of using the condition $\operatorname{idle}(t_1, t) \leq
  \sum_{j=1}^{i-1} x_j S_j$ in Eq.~\eqref{eq:sumof-sigma} to
  replace $\sigma_j$ with $S_j$. But, this will create a serious
  problem in Step 3 later, since we cannot always guarantee that
  $t_i^*\leq t_i$ for $i=1,2,\ldots,k$ in Step 3 if we do so in Step 2. Such a treatment should not be applied at this moment here.}

\noindent\textbf{3.} By the construction of the
schedule $\Psi^1$, we know that $\operatorname{exec}_j(t_1, t_j)=0$ since task $\tau_j$ is not executed between $t_1$ and $t_j$. Therefore, $\operatorname{exec}_j(t_1, t)=0$ if $t<t_j$ and $\operatorname{exec}_j(t_1, t)=\operatorname{exec}_j(t_j, t)$ if $t > t_j$. Since $x_j = 0$ if $\tau_j \in {\bf T}_0$ and $x_j = 1$ if $\tau_j \in {\bf T}_1$, by Lemmas~\ref{lemma:Wj1-exact},~\ref{lemma:Wj0-exact} and ~\ref{lemma:Wj0-dominate}, we have 
for all $t \mid t_{i} \leq t < t_{i+1}$ and $i=1,2,\ldots,k-1$,
{\footnotesize \begin{align}
\label{eq:exec_time}
\operatorname{exec}(t_1, t) & = \sum_{j=1}^{i} \operatorname{exec}_j(t_1, t) = \sum_{j=1}^{i} \operatorname{exec}_j(t_j, t) \nonumber \\
& \leq \sum_{j=1}^{i} \left( x_j\cdot W_j^1(t-t_j)  + (1-x_j)\cdot \widehat{W}_j^0(t-t_j, c_j^*) \right) \nonumber \\
&\leq \sum_{j=1}^{i} \left( x_j\cdot W_j^1(t-t_j)  + (1-x_j)\cdot W_j^0(t-t_j) \right)
\end{align}} 

Therefore, combining Corollary~\ref{lem:bound_idle}, Eq.~\eqref{eq:exec_time} and Eq.~(\ref{eq:exec_plus_idle}), we obtain Eq.~\eqref{eq:exec_plus_idle-2}. 

Moreover, since $\tau_k'$ does not complete its execution
\emph{strictly} before $f_k$ and because, by definition, $\tau_k'$
does not self-suspend, we also know that $\operatorname{idle}(t_k, t) = 0$ for $t_k \leq t < f_k$. Therefore, using Corollary~\ref{lem:bound_idle}, we get for all $t \mid t_k \leq t < f_k$ 
\begin{equation}
\label{eq:idle_time2}
\operatorname{idle}(t_1, t) \leq \sum_{j=1}^{k-1}
x_j \sigma_j.
\end{equation}
Furthermore, by Lemma~\ref{lemma:exec-tau-k}, $\operatorname{exec}_k(t_k, t) < C_k'$ for $t < f_k$. Therefore, adding $\operatorname{exec}_k(t_k, t)$ to Eq.~\eqref{eq:exec_time}, we get for all $t \mid t_{k} \leq t < f_k$ 
{\footnotesize \begin{align}
\label{eq:exec_time2}
\operatorname{exec}(t_1, t) < C_k' + \sum_{j=1}^{k-1} \left( x_j\cdot W_j^1(t-t_j)  + (1-x_j)\cdot W_j^0(t-t_j) \right).
\end{align}}
Combining Eqs.~\eqref{eq:idle_time2}, \eqref{eq:exec_time2} and \eqref{eq:exec_plus_idle}, we obtain Eq.~\eqref{eq:exec_plus_idle-3}.
\end{proof}




\begin{example}
Consider the same 4 tasks as in Example~\ref{ex:proof_step1}, for which a possible schedule was depicted in Figure~\ref{fig:example} when $\epsilon$ is very close to $0$. We have $x_1\sigma_1=1$, $x_2\sigma_2=0$ and $x_3\sigma_3=1-2\epsilon$. The corresponding functions
$W_1^1(t-t_1)$, $W_2^0(t-t_2)$, $W_3^1(t-t_3)$ are illustrated in
Figure~\ref{fig:example-step2} when $\epsilon$ is close to $0$ and $R_2 = 10$. 
%More precisely,
%{\footnotesize \begin{compactitem}
%  \item $W_1^1(t-t_1) = \floor{\frac{t-t_1}{6}}1 + \min\left\{t-t_1-\floor{\frac{t-t_1}{6}}6, 1\right\}$ if $t \geq t_1$.
%  \item $W_2^0(t-t_2) = t-t_2$ if $t_2 \leq t \leq t_2 + 1$ and $W_2^0(t-t_2) = 1 + \floor{\frac{t-t_2-1}{10}} + \min\left\{t-t_2-1-\floor{\frac{t-t_2-1}{10}}10, 1\right\}$ if $t > t_2+1$.
%  \item $W_3^1(t-t_3) = \floor{\frac{t-t_3}{18}}4 + \min\left\{t-t_3-\floor{\frac{t-t_3}{18}}18, 4\right\}$ if $t \geq t_3$.
%\end{compactitem}
%}
As can be seen in Figure~\ref{fig:example-step2}, the inequalities of Eqs.~\eqref{eq:exec_plus_idle-2} and~\eqref{eq:exec_plus_idle-3} clearly hold.\myendproof
\end{example}


Before moving to Step 3, the following lemma is useful for setting the upper bounds of the workload functions.
\begin{Lemma}
  \label{lemma:W_0-and-W_1-upper}
For any $\Delta > 0$, we have
\begin{align}
  W_j^1(\Delta)  \leq &\ceiling{\frac{\Delta}{T_j}}C_j %& \mbox{ if } \tau_j \in {\bf T}_1   
  \label{eq:W_1-upper}\\
  W_j^0(\Delta)  \leq &\ceiling{\frac{\Delta+R_j-C_j}{T_j}}C_j %& \mbox{ if } \tau_j \in {\bf T}_0   
  \label{eq:W_0-upper}
\end{align}
\end{Lemma}
\begin{proof}
  The upper bound of $W_j^1(\Delta)$ is trivial. Therefore, we focus on the upper bound of $W_j^0(\Delta)$. 
  
  If $0 < \Delta \leq C_j$, then by Eq.~\eqref{eq:execution-case2-precise}, $W_j^0(\Delta) = \Delta \leq C_j \leq \ceiling{\frac{\Delta+R_j-C_j}{T_j}}C_j$. 
  
  If $\Delta > C_j$, then by the third and fourth case of Eq.~\eqref{eq:execution-case2-precise}
{\footnotesize \begin{align*}
W_j^0(\Delta) &\leq C_j + W_j^1(\Delta - (T_j-R_j+C_j))\nonumber\\
& \leq C_j+\ceiling{\frac{\Delta-T_j +(R_j-C_j)}{T_j}}C_j  = \ceiling{\frac{\Delta+R_j-C_j}{T_j}}C_j. 
\end{align*}} %where $\leq_1$ is achieved by Lemma~\ref{lemma:Wj0-dominate} and setting
%$c_j^*$ to $C_j$ in Eq.~\eqref{eq:execution-case2-precise}.
\end{proof}

\noindent{\bf Step 3: Creating a Safe Response-Time Upper Bound}





% Lemma~\ref{lemma:conclusion-step2} may seem to provide a way to
% construct the worst-case response time analysis of task
% $\tau_k'$.

The conditions in Lemma \ref{lemma:conclusion-step2}  cannot be used directly since the values $t_j$ ($j=1,2,\ldots,k$)
are unknown in the general case. %We must have a strategy to use the
%conditions in Lemma~\ref{lemma:conclusion-step2} for analyzing a safe upper bound on the response time of $\tau_k'$.
Therefore, Step 3 constructs a safe response-time analysis based on the
conditions specified by Eqs.~\eqref{eq:exec_plus_idle-2} and
\eqref{eq:exec_plus_idle-3} in Lemma~\ref{lemma:conclusion-step2}. Our
goal in this step is to prove that Eq.~\eqref{eq:TDA-suspension-tighter0} in Theorem~\ref{theorem:general-framework} covers all the cases listed
in Lemma~\ref{lemma:conclusion-step2} for any fixed-priority schedule $\Psi^1$ generated from schedule $\Psi$.

Our proof strategy is to first artificially move $t_i$ to $t_i^*$ for $i=1,2,\ldots,k$ such that $t_i^*
\leq t_i$. We define $t_i^*$ as follows:
\begin{equation}
\label{eq:ti*}
t_i^* \equals
\begin{cases}
t_1 & \mbox{if } i=1 \\
t_{i-1}^*+ x_{i-1}\times\sigma_{i-1} & \mbox{if } i=2,3,\ldots,k \\
\end{cases}
\end{equation}
and we prove that $t_i^*$ is indeed smaller than or equal to $t_i$.
\begin{Lemma}
\label{lemma:ti*}
$t_i^* \leq t_i$ for $i=1,2,\ldots,k$.
\end{Lemma}
\begin{proof}
By the definition of $\sigma_i$, we know that $\sigma_i \leq t_{i+1}-t_i$ for $i=1,2,\ldots,k-1$.
Therefore,  for $i=2,3,\ldots,k$,%\footnote{As already mentioned in Footnote~\ref{footnote-why-sigma}, if we used $S_j$ in Eqs.~(\ref{eq:exec_plus_idle-2}) and
%(\ref{eq:exec_plus_idle-3}) instead of $\sigma_j$, this condition would not hold, and the proof would fall apart.}
\begin{equation*}
  t_i = t_1 + \sum_{j=1}^{i-1} (t_{j+1} - t_j) \geq t_1 + \sum_{j=1}^{i-1} \sigma_j \geq t_1 + \sum_{j=1}^{i-1} x_j \sigma_j = t_i^*
\end{equation*}
since $x_j \in \{0, 1\}$ for any $j=1,2,\ldots,i-1$. 
Finally, the property trivially holds for $i=1$. 
\end{proof}

%The property that $t_i^* \leq t_i$ is very important in our proof since this can lead to the conditions in Eqs.~\eqref{eq:execution-case1-shifted},~\eqref{eq:execution-case2-shifted}, etc.

%Figure~\ref{fig:example-proof-final} provides a concrete illustration about the procedure to define $t_1^*, t_2^*, t_3^*, t_4^*$ for the example used in Example~\ref{ex:proof_step1} when $\epsilon$ is close to $0$.
 
 %Moreover, by
%definition, $t_j^*$ is $t_1^* + \sum_{i=1}^{j-1} x_i\cdot\sigma_i$ for
%$j=2,3,\ldots,k$.

\begin{figure}[t]
  \centering

		\def\ux{0.32cm}\def\uy{0.2cm} 
\scalebox{0.8}{
		\begin{tikzpicture}[x=\ux,y=\uy, font=\sffamily,thick]  
		\tikzset{
			task/.style={fill=#1,  rectangle, text height=.3cm},
			task1a/.style={task=green!30},
			task1b/.style={task=green},
			task2a/.style={task=orange!30},
			task2b/.style={task=orange, minimum width=1mm},
			task3a/.style={task=pink, minimum width=1mm},
			task3b/.style={task=pink!80, minimum width=1mm},
			task4a/.style={task=cyan, minimum width=1mm},
			task4b/.style={task=cyan!50},
			task5/.style={task=blue},
			task6/.style={task=purple},
			task7/.style={minimum height=\ux,draw},
			task8/.style={minimum height=\ux,draw,thick},
			task9/.style={task=gray,minimum height=0.7cm,draw},
		}
		\tikzstyle{jobs}=[ fill=black!50];
		
		\draw[->] (0,0) -- coordinate (xaxis) (25,0) node[anchor=west] {$t$};
		\foreach \x in {0,...,24}{
			\draw[-](\x,0.1) -- (\x,-0.1)
			node[below] {\footnotesize $\x$};
			
		}
		\draw[->] (0,0) node[anchor=east] {}-- coordinate (xaxis) (0,18.5);
		\foreach \y in {0,2,...,18}{
			\draw[-](0.1, \y) -- (-0.1, \y)
			node[left] {\small $\y$};
			
		}

		\foreach \y in {1,2,...,18}{
                  \draw[-,very thin,lightgray, dashed](0,\y) -- (24,\y);
                }
		\foreach \x in {4,7,20,18}{
                  \draw[-,very thin,lightgray, dashed](\x,0) -- (\x,18);
                }
                  
                \draw (20, -3) node {\small $f_4$};
                \draw (7, -3) node {\small $t_4$};
                \draw (6, -3) node {\small $t_3$};
                \draw (6, -4) node {\small $t_2$};
                \draw (4, -3) node {\small $t_1$};

                \draw[->,red,thin](4.5, 7) node[anchor=south]{\footnotesize
                 LHS of Eq.~(\ref{eq:exec_plus_idle-2})} -- (5,2);
                \draw[-, very thick, red] (4,0) -- (4, 1) -- (5, 2) -- (6,2) -- (6, 3)
                -- (7, 5);
                \draw[->,red,thin](7, 16) node[anchor=south]{\footnotesize
                 LHS of Eq.~(\ref{eq:exec_plus_idle-3})} -- (11,15.35);
                \draw[-, very thick, red] (7, 10) -- (8, 12) -- (10, 14) -- (11,
                15) -- (16, 15) -- (17, 16) -- (18, 17) -- (20, 17);
                \draw[-, very thick, blue] (7,3) --(20,16);
                \draw[-, very thick, blue] (4,0) --(7,3);

                \draw[-,thick] (4,0) -- (5, 1) -- (10, 1) -- (11, 2) -- (16, 2)
                -- (17, 3) -- (22, 3) -- (23, 4) -- (24, 4);
                \draw[-,dashed] (6,0) -- (7, 1) -- (8, 2) -- (17, 2) -- (18, 3)
                -- (24, 3);
                \draw[-,dotted] (6,0) -- (10, 4) -- (24, 4);

		\end{tikzpicture}    }
  \caption{\small The workload function for the three higher-priority tasks in Example~\ref{ex:proof_step1} when $\epsilon$ is very close to $0$. Solid black line: $W_1^1(t-t_1)$ when $t \geq t_1$, Dashed black line: $W_2^0(t-t_2)$ when $t \geq t_2$, Dotted black line: $W_3^1(t-t_3)$ when $t \geq t_3$, where the three workload functions are $0$ if $t-t_j < 0$ for $j=1,2,3$, Blue line (the only linear function from $t=4$ in this figure): $t-t_1$, Red line (marked by Eq.~\eqref{eq:exec_plus_idle-2} and Eq.~\eqref{eq:exec_plus_idle-3}): left-hand side of Eq.~(\ref{eq:exec_plus_idle-2}) when $t < 7$ and left-hand side of Eq.~(\ref{eq:exec_plus_idle-3}) when $ 7 \leq t < 20$.}
  \label{fig:example-step2}
\end{figure}




\begin{Lemma}
\label{lemma:step-3-one-condition}
$\forall t \mid t_k^* \leq t < f_k$,
\begin{equation}
\label{eq:exec_plus_idle-almost-final} 
C_k'+\sum_{j=1}^{k-1} x_j\cdot W_j^1(t-t_j^*) + (1-x_j)\cdot W_j^0(t-t_j^*) > t-t_k^*.
\end{equation}  
\end{Lemma}
\begin{proof}
Because $t_j \geq t_j^*$ by Lemma~\ref{lemma:ti*}, we have $\forall \Delta \geq 0$
\begin{align}
  \label{eq:execution-case1-shifted}
&  W_j^1(\Delta)  \leq W_j^1(\Delta + (t_j-t_j^*)) \\
  \label{eq:execution-case2-shifted}
&  W_j^0(\Delta)  \leq W_j^0(\Delta + (t_j-t_j^*)).
\end{align}

It results that, for $j=1,2,\ldots,k-1$, $W_j^1(t-t_j)
\leq W_j^1(t-t_j^*)$ and $W_j^0(t-t_j) \leq W_j^0(t-t_j^*)$ for any $t
\geq t_j$. Injecting those two inequalities into 
Eq.~(\ref{eq:exec_plus_idle-2}) $\forall t \mid t_k^* \leq t < t_k$ leads to\footnote{This holds since the interval $[t_k^*, t_k]$ is fully covered by the interval $[t_1, t_k]$.}
{\small \begin{align*}
&\sum_{j=1}^{k-1} x_j\cdot (W_j^1(t-t_j^*)+\sigma_j) + (1-x_j)\cdot W_j^0(t-t_j^*) \geq t-t_1,
\end{align*}}
and because by Eq.~\eqref{eq:ti*}, $t_k^* \equals t_1 + \sum_{j=1}^{k-1}x_j\sigma_j$, we get
{\small \begin{align}
\sum_{j=1}^{k-1} x_j\cdot W_j^1(t-t_j^*) + (1-x_j)\cdot W_j^0(t-t_j^*) \geq t-t_k^*,
\label{eq:exec_plus_idle-4}
\end{align}}%where $\Rightarrow$ is due to
%$t_k^* \equals t_1 + \sum_{j=1}^{k-1}x_j\sigma_j$ by Eq.~\eqref{eq:ti*}.
since $C_k' \geq C_k > 0$, it finally holds that
{\small \begin{align}
C_k' + \sum_{j=1}^{k-1} x_j\cdot W_j^1(t-t_j^*) + (1-x_j)\cdot W_j^0(t-t_j^*) > t-t_k^*.
\label{eq:exec_plus_idle-4-bis}
\end{align}}

Similarly, injecting Eqs.~\eqref{eq:execution-case1-shifted}~and~\eqref{eq:execution-case2-shifted} into 
Eq.~(\ref{eq:exec_plus_idle-3}) $\forall t \mid t_k \leq t < f_k$ leads to 
{\small \begin{equation}
\label{eq:exec_plus_idle-5}
C_k'+\sum_{j=1}^{k-1} x_j\cdot W_j^1(t-t_j^*) + (1-x_j)\cdot W_j^0(t-t_j^*) > t-t_k^*.
\end{equation}}

By Eq.~\eqref{eq:exec_plus_idle-4-bis} (valid for $\forall t \mid t_k^* \leq t < t_k$) and Eq.~\eqref{eq:exec_plus_idle-5} (valid $\forall t \mid t_k \leq t < f_k$), we prove the lemma.
  %This is due to the above discussions for Eqs.~\eqref{eq:exec_plus_idle-4}~and~\eqref{eq:exec_plus_idle-5} and the fact $C_k' > 0$.
\end{proof}

\begin{Lemma}
\label{lemma:step-3-ceiling-condition}
$\forall \theta \mid 0 \leq \theta < f_k-t_k^*$,
\begin{equation}
\label{eq:exec_plus_idle-almost-final-before-replacing-sigma} 
C_k'+\sum_{j=1}^{k-1} \ceiling{\frac{\theta+X_j+(1-x_j)(R_j-C_j)}{T_j}} C_j > \theta,
\end{equation}
where $X_j$ is $\sum_{\ell=j}^{k-1} x_\ell\sigma_\ell$. 
\end{Lemma}
\begin{proof}
  By Eq.~\eqref{eq:ti*}, we have $t_j^* = t_k^* - \sum_{\ell=j}^{k-1}
  x_\ell\sigma_\ell$. Therefore, $\forall t \mid t_k^* \leq t < f_k$,
  we have $t-t_j^* = t - t_k^* + \sum_{\ell=j}^{k-1}
  x_\ell\sigma_\ell = t-t_k^* + X_j$ for every $j=1,2,\ldots,k-1$. 
By using Lemma~\ref{lemma:W_0-and-W_1-upper} and $t-t_j^*$ above, we can rewrite the condition in Lemma~\ref{lemma:step-3-one-condition} as $C_k'+\sum_{j=1}^{k-1} \left(x_j\ceiling{\frac{t-t_k^*+X_j}{T_j}} C_j  + (1-x_j) \ceiling{\frac{t-t_k^*+X_j+R_j-C_j}{T_j}} C_j\right) > t-t_k^*$, $\forall t \mid t_k^* \leq t < f_k$.
Since $x_j$ is either $0$ or $1$, this is equivalent to  $\forall t \mid t_k^* \leq t < f_k$,
\begin{align*}
C_k'+\sum_{j=1}^{k-1} \ceiling{\frac{t-t_k^*+X_j+(1-x_j)(R_j-C_j)}{T_j}} C_j > t-t_k^*
\end{align*}

  By replacing $t-t_k^*$ with $\theta$, we reach the conclusion.
\end{proof}

% Therefore, for task $\tau_j$ in ${\bf T}_1$, we know that $\forall t
% \mid t_k^* \leq t < f_k$
% \begin{equation}\label{eq:upper-bound-Wj1}
% W_j^1(t-t_j^*) \leq \ceiling{\frac{t-t_j^*}{T_j}}C_j =
% \ceiling{\frac{t-t_k^* +\sum_{\ell=j}^{k-1} x_\ell\sigma_\ell}{T_j}}C_j  
% \end{equation}
%  Moreover, for task $\tau_j$ in ${\bf T}_0$ with $x_j = 0$,
% $\forall t
% \mid t_k^* \leq t < f_k$, we have 
% {\small \begin{align}
% W_j^0(t-t_j^*) & = W_j^0(t-t_k^* + t_k^* - t_j^*) = W_j^0(t-t_k^* + \sum_{\ell=j}^{k-1} x_\ell\sigma_\ell)\nonumber\\
% &\leq C_j + W_j^1(t-t_k^* + \sum_{\ell=j}^{k-1} x_\ell\sigma_\ell-(T_j-R_j+C_j))\nonumber\\
% & \leq \ceiling{\frac{t-t_k^*+\sum_{\ell=j}^{k-1} x_\ell\sigma_\ell +(R_j-C_j)}{T_j}}C_j  \label{eq:upper-bound-Wj0}
% \end{align}} Therefore, we can conclude that 
% $\forall t \mid t_k^* \leq t < f_k$
% \begin{equation}
% C_k'+\sum_{j=1}^{k-1} \ceiling{\frac{t-t_k^*+X_j+(1-x_j)(R_j-C_j)}{T_j}} C_j > t-t_k^*,
% \end{equation}
% where $X_j$ is $\sum_{\ell=j}^{k-1} x_\ell\sigma_\ell$. We replace $t-t_k^*$ with $\theta$. 

\begin{appProof}{Theorem \ref{theorem:general-framework}} 
The condition in Lemma~\ref{lemma:step-3-ceiling-condition} implies that the minimum $\theta$ with $\theta > 0$ and
$C_k'+\sum_{j=1}^{k-1} \ceiling{\frac{\theta+X_j+(1-x_j)(R_j-C_j)}{T_j}} C_j = \theta$
 is larger than or equal to $f_k-t_k^* \geq f_k-t_k$ and therefore provides an upper bound on any job $J_k$ released in any schedule $\Psi$. 
However, the condition in Lemma~\ref{lemma:step-3-ceiling-condition}
still requires the knowledge of $\sigma_i$. Yet, it is straightforward to see
that $\sum_{j=1}^{k-1}
\ceiling{\frac{\theta+X_j+(1-x_j)(R_j-C_j)}{T_j}} C_j$ is maximized when $X_j$ is the largest. Since by Lemma~\ref{lem:max_idle} $X_j = \sum_{\ell=j}^{k-1} x_\ell\sigma_\ell \leq \sum_{\ell=j}^{k-1} x_\ell S_\ell = Q_j^{\vec{x}}$, we reach the conclusion of the correctness
of Theorem~\ref{theorem:general-framework} when replacing $X_j$ with $Q_j^{\vec{x}}$ in the previous equation.
\end{appProof}

\begin{figure}[t]
  \centering

		\def\ux{0.32cm}\def\uy{0.2cm} \scalebox{0.8}{
		\begin{tikzpicture}[x=\ux,y=\uy, font=\sffamily,thick]  
		\tikzset{
			task/.style={fill=#1,  rectangle, text height=.3cm},
			task1a/.style={task=green!30},
			task1b/.style={task=green},
			task2a/.style={task=orange!30},
			task2b/.style={task=orange, minimum width=1mm},
			task3a/.style={task=pink, minimum width=1mm},
			task3b/.style={task=pink!80, minimum width=1mm},
			task4a/.style={task=cyan, minimum width=1mm},
			task4b/.style={task=cyan!50},
			task5/.style={task=blue},
			task6/.style={task=purple},
			task7/.style={minimum height=\ux,draw},
			task8/.style={minimum height=\ux,draw,thick},
			task9/.style={task=gray,minimum height=0.7cm,draw},
		}
		\tikzstyle{jobs}=[ fill=black!50];
		
		\draw[->] (0,0) -- coordinate (xaxis) (25,0) node[anchor=west] {$t$};
		\foreach \x in {0,...,24}{
			\draw[-](\x,0.1) -- (\x,-0.1)
			node[below] {\footnotesize $\x$};
			
		}
		\draw[->] (0,0) node[anchor=east] {}-- coordinate (xaxis) (0,18.5);
		\foreach \y in {0,2,...,18}{
			\draw[-](0.1, \y) -- (-0.1, \y)
			node[left] {\small $\y$};
			
		}

		\foreach \y in {1,2,...,18}{
                  \draw[-,very thin,lightgray, dashed](0,\y) -- (24,\y);
                }
		\foreach \x in {4,7,20,18}{
                  \draw[-,very thin,lightgray, dashed](\x,0) -- (\x,18);
                }
                  
                \draw (20, -3) node {\small $f_4$};
                \draw (6, -3) node {\small $t_4^*$};
                \draw (5, -3) node {\small $t_3^*$};
                \draw (5, -4.5) node {\small $t_2^*$};
                \draw (4, -3) node {\small $t_1^*$};

                \draw[->,red,thin](5.5, 4) node[anchor=south,rotate=45,yshift=-0.1cm]{\footnotesize
                 LHS of Eq~\eqref{eq:exec_plus_idle-4}} -- (6.2,3.5);
                \draw[-,  very thick, red] (6, 3) -- (7,5);
                \draw[->,red,thin](8.4, 16.2) node[anchor=south]{\footnotesize
                 LHS of Eq.~(\ref{eq:exec_plus_idle-5}) and Eq.~\eqref{eq:exec_plus_idle-almost-final} when $t \geq 7$} -- (8,11.2);
                \draw[-, very thick, red] (7, 10) -- (9, 12) -- (10, 12) -- (11, 13) 
                -- (16, 13) -- (17, 15) -- (20, 15);
                \draw[-, very thick, blue] (6,0) --(20,14);
                \draw[-, very thick, cyan] (6,8) --(7,10);
                \draw[->,cyan](5.6, 10) node[anchor=south, rotate=45,xshift=-0.5cm, text width=2.5cm]{\footnotesize
                 LHS of Eq.~\eqref{eq:exec_plus_idle-almost-final} when $6 \leq t < 7$} -- (6.4,8.95);


               \draw[color=gray, dotted, very thick](6, 11) -- (7, 11) -- (7, 12) -- (10, 12) -- (10, 13) -- (16, 13) -- (16, 15) -- (20, 15);
               \draw[->,gray](17, 16.4) node[anchor=west,text width=2.5cm]{\footnotesize  LHS of Eq.~\eqref{eq:exec_plus_idle-almost-final-before-replacing-sigma} when $t=\theta+6$} -- (16,15.2);


                \draw[-,thick] (4,0) -- (5, 1) -- (10, 1) -- (11, 2) -- (16, 2)
                -- (17, 3) -- (22, 3) -- (23, 4) -- (24, 4);
                \draw[-,dashed] (5,0) -- (6, 1) -- (7, 2) -- (16, 2) -- (17, 3)
                -- (24, 3);
                \draw[-,dotted] (5,0) -- (9, 4) -- (23, 4) -- (24, 5);

		\end{tikzpicture}    }
  \caption{\small The workload function for the three higher-priority tasks in Example~\ref{ex:proof_step1} when $\epsilon$ is very close to $0$. Solid black line: $W_1^1(t-t_1^*)$ when $t \geq t_1^*$, Dashed black line: $W_2^0(t-t_2^*)$ when $t \geq t_2^*$, Dotted black line: $W_3^1(t-t_3^*)$ when $t \geq t_3^*$, where the three workload functions are $0$ if $t-t_j^* < 0$ for $j=1,2,3$, Blue line (the only linear function from $t=6$ in this figure): $t-t_k^*=t-6$, Red line (marked by Eq.~\eqref{eq:exec_plus_idle-4} and Eq.~\eqref{eq:exec_plus_idle-5}): left-hand side of Eq.~(\ref{eq:exec_plus_idle-4}) when $t < 7$ and left-hand side of Eq.~(\ref{eq:exec_plus_idle-5}) and Eq.~\eqref{eq:exec_plus_idle-almost-final} when $ 7 \leq t < 20$, Purple line (marked by Eq.~\eqref{eq:exec_plus_idle-almost-final} when $6 \leq t < 7$), Gray dotted line (marked by Eq.~\eqref{eq:exec_plus_idle-almost-final-before-replacing-sigma}) by setting $\theta=t-6$.}
  \label{fig:example-proof-final}
\end{figure}


%\hfill {\bf end of the proof of Theorem~\ref{theorem:general-framework}.} \qed  

\begin{example}
  We consider Example~\ref{ex:proof_step1} when $\epsilon$ is very
  close to $0$ to illustrate Step 3 in the proof of
  Theorem~\ref{theorem:general-framework}. For such a case, $t_1^*=4,
  t_2^*=5, t_3^*=5$, and $t_4^*=6$.
  Figure~\ref{fig:example-proof-final} presents the corresponding
  relations of the inequalities in Step 3. 
% be demonstrated in Figure~\ref{fig:example-proof-final} based on the
% previous example in
% Figure~\ref{fig:example}. Figure~\ref{fig:example-proof-final}
% provides the imaginary workload and an imaginary execution plan based
% on the test behind the condition in
% Eq.~\eqref{eq:exec_plus_idle-almost-final}. \emph{Note that this is
%   not an actual schedule since task $\tau_2$ is artificially alerted
%   to release two jobs within a short time interval. This is only for
%   illustrative purposes.}
% The two idle time units are used
% between time $4$ and time $6$. The accumulated workload is then
% started to be executed at time $6$ and the processor does not
% idle after time $6$. Over here, we see that two jobs of task $\tau_2$ are executed
% back to back from time 7 to time 9. As
% shown in the imaginary schedule in
% Figure~\ref{fig:example-proof-final}, the processor is busy executing
% the workload from time $6$ to time $21$, which is more pessimistic
% than the actual schedule in Figure~\ref{fig:example}. 
As shown in Figure~\ref{fig:example-proof-final}, all the inequalities 
  in Eqs.~\eqref{eq:exec_plus_idle-4},~\eqref{eq:exec_plus_idle-5},~\eqref{eq:exec_plus_idle-almost-final}, and~\eqref{eq:exec_plus_idle-almost-final-before-replacing-sigma} hold.
%The conclusion we have in the final statement of the example is that $20-7=f_k-r_k \leq  f_k-6 < 21-6$.
\myendproof
\end{example}
%\vspace{-0.3in}
%\end{appProof}
\ifbool{techreport}{The physical meaning of the above analysis in Theorem 1 can be found in Appendix\citetechreport{}.}{}

As mentioned at the beginning of this section, all the lemmas and corollaries proven above are valid for any job in any schedule $\Psi$ where all the jobs respect their deadlines. Yet, those results are still valid for the first job of task $\tau_k$ that misses a deadline in a schedule $\Psi$ (if such a job exists). This allows us to prove Theorem \ref{theorem:general-framework-not-feasible} below.

\begin{appProof}{Theorem \ref{theorem:general-framework-not-feasible}}
  By the assumption that $R_k > T_k$, there exists a schedule $\Psi$
  such that the response time of at least one job of $\tau_k$ is strictly larger than
  $T_k$. Let $J_k$ be the \emph{first} job in the schedule $\Psi$ that has
  response time larger than $T_k$. Suppose that $J_{k}$ arrives at
  time $r_k$. When job $J_k$ is released at time $r_k$, there is no
  other unfinished job of task $\tau_k$. By
  Lemma~\ref{lemma:remove-same-task}, we can safely remove all the
  other jobs of task $\tau_k$ arrived before $r_k$ without affecting
  the response time of $J_k$. It is rather straightforward to see that
  removing all the other jobs of task $\tau_k$ arrived after $r_k$
  also does not change the fact that $J_k$ finishes after
  $r_k+T_k$. Let $f_k$ be the time at which $J_k$ finishes in the
  above schedule after removing the other jobs of task $\tau_k$. We
  know that $f_k - r_k > T_k$.

  Then, we can follow all the procedures and steps in the proof of
  Theorem~\ref{theorem:general-framework} to reach the same conclusion
  in Lemma~\ref{lemma:step-3-ceiling-condition}, which implies
  Theorem~\ref{theorem:general-framework-not-feasible} by setting $X_j
  \leq Q_j^{\vec{x}}$ for $j=1,2,\ldots,k-1$ since $f_k-r_k > T_k$ and
  $C_k'=C_k+S_k$.
\end{appProof}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master.tex"
%%% End:
